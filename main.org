#+TITLE: Cadenas de Markov
#+AUTHOR: Ignacio Cordón Castillo
#+OPTIONS: toc:nil
#+STARTUP: indent
#+STARTUP: latexpreview

#+latex_class: scrreprt
#+latex_class_options: [oneside,english,openright,titlepage,numbers=noenddot,openany,headinclude,footinclude=true,cleardoublepage=empty,abstractoff,BCOR=5mm,paper=a4,fontsize=12pt,ngerman,american]
#+latex_header_extra: \definecolor{ugrColor}{HTML}{c6474b}
#+latex_header_extra: \input{frontmatter/titlepage}\usepackage{wallpaper}\ThisULCornerWallPaper{1}{imgs/ugrA4.pdf}
#+latex_header_extra: \usepackage[T1]{fontenc}
#+latex_header_extra: \usepackage{babel}
#+latex_header_extra: \usepackage[normalem]{ulem}
#+latex_header_extra: \usepackage[beramono,eulerchapternumbers,linedheaders,parts,a5paper,dottedtoc,manychapters]{classicthesis}
#+latex_header_extra: \input{config/setup}
#+latex_header_extra: \input{config/classicthesis}
#+latex_header: \input{config/macros}

\newpage
* Authenticity declaration :ignore:
:PROPERTIES:
:UNNUMBERED: t
:END:

\thispagestyle{empty}

\vspace*{3cm}

\begin{center}
\textsc{Declaración}
\end{center}

\vspace*{3.5cm}

En cumplimiento de la normativa aprobada en Consejo de Gobierno de 4 de marzo de 2013,
sobre Directrices de la Universidad de Granada para el desarrollo de la asignatura "Trabajo Fin
de Máster" de sus títulos de máster (Art 8,4)

\vspace*{1cm}
\begin{center}
\Large \textbf{D.D\textordfeminine} \dotuline{\textsc{Ignacio Cordón Castillo}\hspace*{\fill}} \normalsize
\end{center}

\vspace*{1cm}

Asume la originalidad del trabajo fin de máster, entendida en el sentido de que no ha utilizado
fuentes sin citarlas debidamente.

\vspace*{1cm}
Granada, a \septdate\today

\begin{flushright}
 \begin{tabular}{m{5.5cm}}
     \img{imgs/firma_nacho.png}{0.45} \\
     Fdo: Ignacio Cordón Castillo
 \end{tabular}
\end{flushright}

* Acknowledgments
:PROPERTIES:
:UNNUMBERED: t
:END:

I would like to thank my family and my friends, because they are always and important part of what I do,
providing me with great support.

Also, many thanks to Giorgio, my tutor, since he always kept an open mind when it
came to me proposing work to do on his package. He encouraged me to try different approaches and I had
plenty of proposals from his side to work on. I have had plenty of good challenges to dive in, learn and
apply new knowledge thanks to him.

* Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

Las cadenas de Markov son un área de estudio que comprende conocimientos de, por nombrar algunas,
probabilidad, inferencia, álgebra lineal, grafos o teoría de algoritmos (puesto que las cadenas discretas
pueden ser simuladas en un ordenador). Por todo lo expuesto representan un campo de
estudio interesante desde el punto de vista matemático e informático, y las hacen una temática ideal desde
el punto de vista de la interdisciplinariedad del máster Fisymat.

En el capítulo [[Markov chains]] se presenta una revisión teórica a las cadenas de Markov en tiempo discreto,
centrándonos especialmente en teoremas y propiedades relacionados con un número finito de estados: definición,
ecuación de Chapman-Kolmogorov, tiempos y probabilidades de llegada, clasificación de estados,
incluyendo clases comunicantes, transitoriedad, recurrencia y estados absorbentes, cadenas de Markov
ergódicas y regulares y su relación con las distribuciones estacionarias, tiempos medio de llegada y de
recurrencia, \ldots

La sección [[=markovchain= package]] describe el trabajo hecho en el paquete de R =markovchain=. Esta librería
es un proyecto de sofware libre que permite analizar cadenas de Markov discretas desde un punto
de vista probabilístico y estadístico (inferencia). El autor de este trabajo ha analizado la parte del análisis
probabilístico que este paquete proporciona y se han identificado los puntos clave donde los algoritmos podrían
ser mejorados, desde el punto de vista de la corrección matemática y la eficiencia algorítmica. También se han
añadido nuevas colecciones de algoritmos. Tanto los procedimientos refactorizados como los añadidos están relacionados
con los contenidos teóricos descritos en el capítulo anterior, y han sido abordados gracias a los conocimientos
matemáticos adquiridos a través de la bibliografía.

Los algoritmos mejorados / reprogramados oscilan desde la identificación de clases comunicantes, pasando por
el cálculo de los tiempos medios de llegada o la descripción de clases recurrentes y transitorias. Como
consecuencia de ellos, ha sido redise\-ñada la clasificación de estados de una manera más eficiente, así como
la transformación a forma canónica. Además, el algoritmo del cálculo de distribuciones estacionarias ha sido
altamente mejorado, puesto que el antiguo podía devolver resultados incorrectos. Las nuevas incorporaciones
versan principalmente sobre cantidades relacionadas con la absorción (tiempo medio de absorción, probabilidad
de absorción), número esperado de visitas a un estado, el tiempo medio de recurrencia, las probabilidades de
llegada, el cálculo eficiente de la accesibilidad a estados, de potencias y la clasificación de cadenas
(regulares, ergódicas, no ergódicas).

En el capítulo [[Conclusions and future work]] se esbozan ideas para trabajo futuro en el sofware, a tenor del
conocimiento ganado durante el estudio teórico y práctico de la librería, que en conjunción deberían llevar
a la publicación de dicho software en una revista académica de alto impacto.

El anexo [[Appendix]] incluye instrucciones sobre cómo instalar y usar el sofware. Asimismo, se explican otras
labores realizadas en el marco de este proyecto: incorporación de /tests/ a la librería basados en la
comprobación de propiedades matemáticas, revisión de documentación así como el diseño de una página web
para incrementar la visibilidad del paquete y mejorar el acceso a la documentación del mismo.

\vspace*{1cm}
\textsc{Palabras clave}: Cadenas de Markov, algoritmos, probabilidad, álgebra lineal, grafos

\vspace*{1cm}
\begin{center}\rule{10cm}{1pt}\end{center}
\vspace{1cm}

Markov chains are a mathematical area which encompasses, to name a few, probability, inference, linear
algebra, gaphs or algorithms theory (since discrete markov chains can be simulated in a computer). All of
the aforementioned conditions make them an interesting study from both the mathematical and computer science
viewpoint, and therefore a perfect topic from the interdisciplinary nature of Fisymat master's degree.

In chapter [[Markov chains]] we present a theoretical review of discrete time Markov chains, focusing mostly
on theorems and properties which imply a finite number of states: definition, Chapman-Kolmogorov equation,
hitting times and probabilities, classification of states, including communicating classes,
transience, recurrence and absorption, ergodic and regular Markov chains and their relationship with
steady states, mean first passage and recurrence times, \ldots

Section [[=markovchain= package]] describes the work done in =markovchain= R package. This software library
comprises open source utilities which allow to analyze discrete Markov chains from a probabilistic
and statistical (inference) perspective. The author of the present work analyzed the discrete time probabilistic
analysis provided by this package and identified key points where the algorithms could be improved, both from
the mathematical correctness perspective and from the computational algorithmic efficiency. Also a collection
of new algorithms were added. All those procedures (both the newly added and the refactored ones) are
related to the theoretical contents described in the previous chapter, and have been tackled thanks to the
mathematics knowledge acquired from the bibliography.

The improved / recoded algorithms range from communicating classes identification, to mean first passage
times computation or recurrent (resp. transient) classes description. Also as a consequence of those, state
classification has been redesigned in a more efficient manner, as well as conversion to canonic form. Moreover,
steady states algorithm has been highly improved, due to the fact that incorrect results could arise
from the former one. The added algorithms are mainly related to absorption related quantities (mean
aborption time, absorption probability), mean number of visits to a state, mean recurrence times, hitting
probabilities, efficient reachability, power computation and Markov chain classification
(regular, ergodic or non ergodic).

In chapter [[Conclusions and future work]] we outline future work that could be done in the package, in light
of the knowledge we have gathered during the theoretical and practical study of the library, and that should
cumminate with its publication in a high impact journal.

Appendix [[Appendix]] includes instructions on how to install and use the software. Furthermore, it explains more
work developed within this project: addition of tests for the libary based in check of mathematical
properties, review of documentation and design of a web page to increase the visibility of the
sofware and improve access to its documentation.

\vspace*{1cm}
\textsc{Keywords}: Markov chains, algorithms, probability, linear algebra, graphs
* Table of contents :ignore:
:PROPERTIES:
:UNNUMBERED: t
:END:
#+TOC: headlines 3
* Introduction

To fully understand this work we assume previous notions of probability, programming and familiarity with big
$\mathcal{O}$ algoritmic notation [fn::[[https://en.wikipedia.org/wiki/Big_O_notation][https://en.wikipedia.org/wiki/Big_O_notation]]].

Markov chains are still nowadays a highly researched area of mathematics and computer science which is used
in a wide variety of fields: weather prediction, actuarial science, algorithms (for example used in Markov
chain Monte Carlo methods), physics, speech recognition, genetics, \ldots. We focus in this work on discrete
Markov chains with a finite state space, which becase of finiude are the most natural Markov chain we can
represent in a computer.

Among programming languages nowadays, R is probably the most used ones among statisticians and the main
competitor of =python= in machine learning programming. The =markovchain= package
[fn::[[https://github.com/spedygiorgio/markovchain][https://github.com/spedygiorgio/markovchain]]] is an open source library available
at the official R packages repository, CRAN [fn::[[https://cran.r-project.org/web/packages/index.html][https://cran.r-project.org/web/packages/index.html]]], which
aims to provide easy representations, plotting and analysis of discrete time Markov chains. It is placed in
91st percentile in terms of total downloads (compared to rest of packages of CRAN), and its trend of
downloads continues to be in the 90th monthly percentile as it can be observed in the figure [[fig:mc-downloads]] (approximately 4000 downloads a month [fn::[[https://cranlogs.r-pkg.org/badges/markovchain][https://cranlogs.r-pkg.org/badges/markovchain]]]). We should point
out that package is not only written in R. The core parts are coded in C++, since it is faster than R.

Being that =markovchain= is widely used, a lot of methods could be improved from the mathematical correctness
perspective (for example the computation of steady states could return a linearly dependent base of the
convex space of steady states, which is not correct) and the computational efficiency one (use less
iterative methods and more algebraic ones, improve efficiency for methods in general). Effiency is extremely
important because it can be the difference between being able to analyze a Markov chain with a $10\times 10$
transition matrix or a $100\times 100$ one. Also the package was lacking methods to perform a more complete
analysis of Markov chains and answer possible questions like: what is the probability of ever hitting that
state? (whatever the state represents: bankruptcy in case of actuarial analysis, a tornado in case of
wheather prediction, etc), or how many times are we going to pass through a certain state if initial
conditions for the system are these ones? Even small contributions to the software could impact a lot of
users and researchers.

#+header: :width 5 :height 5 :R-dev-args
#+begin_src R :session examples :results output graphics :file imgs/percentiles.svg :exports results :cache yes :eval no-export
library("packageRank")
library("ggplot2")

start <- as.Date("01-08-19",format="%d-%m-%y")
end <- as.Date("25-08-19",format="%d-%m-%y")
current <- start
downloads <- c()
percentile <- c()

while (current <= end) {
  data <- packageRank(packages = "markovchain", date = current)$package.data
  downloads <- c(downloads, unname(unlist(data["downloads"])))
  percentile <- c(percentile, unname(unlist(data["percentile"])))
  current <- current + 1
}


ndays <- as.numeric(end - start)
downloads <- data.frame(percentile = percentile, date = as.Date(0:(ndays - 1), origin = start))
ggplot(downloads, aes(x = date, y = percentile)) +
  geom_line(color = "darkblue", size = 2) +
  expand_limits(y=30)
#+end_src

#+caption: Percentage of daily downloads of markovchain (packages in CRAN) in August 2019
#+name: fig:mc-downloads
#+attr_latex: :width 9cm
#+RESULTS:
[[file:imgs/percentile.svg]]

The goals set for this work were:

+ Get familiarized with the package and the codebase of it. Identify points where it could be improved.
+ Study opportunities of improvement from the mathematical perspective to ensure correctness.
+ Come up with algorithms related to mathematical contents.
+ Code them and integrate them in the package.
+ Send updates for the package in the official CRAN R repository.

All objectives allow for an incremental work and are presented in inverted order in two chapters: as opposed
to practical driving theory study, the work is presented backwards, with theoretical reviews presented
first. To summarize, what we intend to do is mathematical driven scientific programming.

* Markov chains
This section is based in \cite{velez}, although other theoretical references such as \cite{takis}
or \cite{zitkovic} have been used. Ergodic Markov chains section is based on \cite{horn} and
\cite{grinstead}. Mean first passage time section is mainly based in \cite{grinstead}.

** Definition
For convenience, unless otherwise indicated, we will consider $0\in \mathbb{N}$.

#+begin_definition
A stochastic random process $\{X_n\}_{n\in I}$, where $I\subseteq \mathbb{N}$, is a Markov chain iff the future process $\{X_k\}_{k > n, k \in I}$ is independent of the past process $\{X_k\}_{k < n, k \in I}$ conditionally on $X_n$.
#+end_definition

#+begin_definition
Given $\{X_n\}_{n\in I}$ a Markov chain, let us call the state space the set $S = \bigcup_{n \in I} Img \left(X_n\right)$, i.e. the set where the Markov chain takes its values.
#+end_definition

We will omit the subscript $n\in I$ and assume $I = \mathbb{N}$ henceforth. We will also assume $S$ is finite.

#+begin_lemma
$\{X_n\}_n$ is a Markov chain iff for all $n\in\mathbb{N}$ and given arbitrary $s_0, \ldots, s_{n + 1} \in S$, then it holds:

\[
P(X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0) = P(X_{n + 1} = s_{n + 1} | X_n = s_n)
\]
#+end_lemma

#+begin_proof

From left to right. By Markov chain definition, if $\{X_k\}_{k \in I}$ is independent of the past process $\{X_k\}_{m \in I}$ conditionally on $X_n$, then:

\begin{align*}
&P(X_{n + 1} = s_{n + 1}, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n) = \\
&P(X_{n + 1} = s_{n + 1} | X_n = s_n) \cdot P(X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)
\end{align*}

If we prove that

\[
\frac{P(X_{n + 1} = s_{n + 1}, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)}{P(X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)} = P(X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0)
\]

then we would have arrived at our proof. That derives from:

\begin{align*}
\frac{P(X_{n + 1} = s_{n + 1}, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)}{P(X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)} &=
\frac{P(X_{n + 1} = s_{n + 1}, X_n = s_n, \ldots, X_0 = s_0)}{P(X_n = s_n, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0)} = \\
&= P(X_{n + 1}=s_{n + 1} | X_{n} = s_{n}, \ldots, X_0 = s_0)
\end{align*}

From right to left. We want to prove that:

\begin{align*}
&P(X_{n + m} = s_{n + m}, \ldots X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0) =\\
&P(X_{n + m} = s_{n + m}, \ldots, X_{n + 1} = s_{n + 1} | X_n = s_n)
\end{align*}

On the one hand, if the right part holds, it easy to prove that:

\begin{align*}
& P(X_0 = s_0, \ldots X_n = s_n ) =\\
& P(X_0 = s_0) \cdot P(X_1 = s_1 | X_0 = s_0) \cdot P(X_2 = s_2 | X_1 = s_1) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\end{align*}

substituting each:

\[
P(X_i = s_i | X_{i - 1} = s_{i - 1}) = P(X_i = s_i | X_{i - 1} = s_{i - 1}, \ldots, X_0 = s_0) = \frac{P(X_i = s_i, \ldots, X_0 = s_0)}{P(X_{i - 1} = s_{i - 1}, \ldots, X_0 = s_0)}
\]


Summing over all $s_0, \ldots s_{k - 1}$, we get:

\[
P(X_{k} = s_{k}, \ldots, X_n = s_n) = P(X_k = s_k) \cdot P(X_{k + 1} = s_{k + 1} | X_k = s_k) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\]

Therefore,

\begin{align*}
& P(X_{n + m} = s_{n + m}, \ldots, X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0) =\\
&= \frac{P(X_{n + m}, \ldots, X_0)}{P(X_{n + m - 1}, \ldots, X_0)} \cdot \frac{P(X_{n + m - 1}, \ldots, X_0)}{P(X_{n + m - 2}, \ldots, X_0)} \cdots \frac{P(X_{n + 1}, \ldots, X_0)}{P(X_{n}, \ldots, X_0)} = \\
&= P(X_{n + m} | X_{n + m - 1}) \cdots P(X_{n + 1} | X_{n}) = \frac{P(X_{n + m} = s_{n + m}, \ldots X_{n + 1} = s_{n + 1})}{P(X_n = s_n)} =\\
&= P(X_{n + m} = s_{n + m}, \ldots, X_{n + 1} = s_{n + 1} | X_n = s_n)
\end{align*}
#+end_proof

** Chapman-Kolmogorov equation
As stated before, in the right to left implication (that now we can assume as characterization of
Markov chains), we have shown that transition probabilities can be expressed as:

\begin{align}
& P(X_0 = s_0, \ldots X_n = s_n) = \nonumber \\
& P(X_0 = s_0) \cdot P(X_1 = s_1 | X_0 = s_0) \cdot P(X_2 = s_2 | X_1 = s_1) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\label{eq:trans-probs}
\end{align}

and also:

\begin{align}
& P(X_{m} = s_{m}, \ldots, X_n = s_n | X_{m - 1} = s_{m - 1})) = \nonumber \\
& = P(X_{m} = s_{m} | X_m = s_m) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\label{eq:trans-cond-probs}
\end{align}

#+begin_definition
Given $\{X_n\}_{n\in I}$ a Markov chain with finite state space $S$, let us define the initial distribution as $\mu(s) = P(X_0 = s), s \in S$.
#+end_definition

#+begin_definition
Given $\{X_n\}_{n\in I}$ a Markov chain with finite state space $S$, let us define the transition probabilities as

\begin{align*}
& p_{s, t}(m, n) = P(X_n = t | X_m = s),\quad s,t \in S, \quad n,m \in \mathbb{N}, n\neq m \\
& p_{s, t}(n, n) = \mathbb{1}_{s = t}
\end{align*}
#+end_definition

Therefore, we can rewrite Equation \eqref{eq:trans-probs} as:

\[
P(X_k = s_k, k \le n) = \mu(s_0) \cdot p_{s_0, s_1}(0, 1) \cdot p_{s_1, s_2}(1, 2) \cdots p_{s_{n - 1}, s_n}(n - 1, n)
\]

Summing over $m - 1, \ldots, n -1$ in Equation \eqref{eq:trans-cond-probs}, we deduce:

\begin{equation}
p_{s, t}(m, n) = \sum_{s^{(1)} \in S} \ldots \sum_{s^{(n - m - 1)} \in S} p_{s, s^{(1)}}(m, m + 1) \cdots p_{s^{(n - m - 1)}, t}(n - 1, n)
\label{eq:sum-probs}
\end{equation}

#+begin_definition
We will define the matrix $P(m, n) = (p_{s, t}(m, n))_{s, t \in S}$ henceforth.
#+end_definition

#+begin_proposition
$P$ verifies the semigroup property, best known as Chapman-Kolmogorov equation:

\[
P(m, n) = P(m, l) \cdot P(l, n), \quad \forall m \le l \le n
\]
#+end_proposition
#+begin_proof
Straightforward from Equation \eqref{eq:sum-probs}
#+end_proof

** Time-homogeneous Markov chains
#+begin_definition
Given a Markov chain $\{X_n\}$ with finite space state $S=\{0, \ldots, k\}$, it is said to be a time-homogeneous Markov chain iff it holds:

\[
\forall s, t\in S, \forall n \in \mathbb{N} \qquad p_{s, t} (n, n + 1) = p_{s, t} \in [0, 1]
\]

(i.e. the transition probabilities do not depend on the time).

$p_{s, t}$ is called one-step transition probability and $P = (p_{s, t})_{s, t \in S}$ the transition matrix.
#+end_definition

#+begin_proposition
It holds $P(m, n) = P^{n - m}$
#+end_proposition

#+begin_proof
Trivial
#+end_proof


#+begin_proposition
Naming $\mu_n = \left(\mu_n(0), \ldots, \mu_n(k)\right)$, it holds:

\[
\mu_n = \mu_0 \cdot P^n
\]
#+end_proposition

#+begin_proof
Trivial from Equation \eqref{eq:trans-probs}
#+end_proof

#+begin_definition
We define the law of a Markov chain $\{X_n\}$, where $X_n: \Omega \longrightarrow \mathbb{S}$, as the probability distribution of $X: \Omega \longrightarrow S^{\mathbb{N}}$, where $X(\omega) = \{X_n(w)\}_{n \in \mathbb{N}}$.
#+end_definition

#+begin_definition
We will denote:

\[
P_s(A) = P(A | X_0 = s)
\]

for any given state $s\in S$. Let's note that $P_s$ is univocally defined by $P_s(X_k = t) \quad \forall k \ge 0, t \in S$.

We will also denote

\[
\mathbb{E}_s Y = \mathbb{E}(Y | X_0 = s) = \sum_y y P(Y = y | X_0 = s) = \sum_y y P_i(Y = y)
\]
#+end_definition

** Stationarity
#+begin_definition
We say that $\{X_n\}$ is stationary iff $\{X_n\}_{n \ge 0}$ has the same law as $\{X_n\}_{n \ge m}$.
#+end_definition

#+begin_proposition
A Markov chain $\{X_n\}$ is stationary iff it is time homogeneous and all the $X_k$ have the same
distribution.
#+end_proposition

#+begin_proof
From right to left, if $\{X_n\}$ is stationary, $P(X_0 = s) = P(X_m = s)$ for all $s\in S, m\in \mathbb{N}$
#+end_proof

We will assume all the Markov chains as time-homogeneous ones henceforth, unless stated
otherwise.
** Stopping times and strong Markov property
#+begin_definition
Given $\{X_n\}$ a Markov chain, defined over $(\Omega, \mathcal{F}, P)$, then a stopping time is a random variable
$\tau : \Omega \mapsto \mathbb{N} \bigcap \{\infty\}$ such that:

\[
\{\tau = n \} \in \mathcal{F}_n = \sigma(X_0, X_1, \ldots, X_n), \quad \forall n \in \mathbb{N}
\]

Moreover, we call the $\sigma$ -algebra of events previous to $\tau$, namely $\mathcal{F}_{\tau}$, the one which contains all
the events $A \in \mathcal{F}$ following:

\[
A\bigcap \{\tau = n \} \in \mathcal{F}_n
\]
#+end_definition

For example, in the case of a coin, the first instant (or coin toss) $\tau$ where we get 4 heads
in a row, is a valid stopping time, because we only have to account for the previous random
variables; whereas the first toss where we are just one toss away from having obtained 4 heads
is not a valid stopping time, since it involves future events.

Given $S = \{i_1, i_2, \ldots\}$ we would denote $S^m = \{(i^{(1)}, \ldots, i^{(m)}): i^{(j)} \in S\}$.

#+begin_proposition
Let $\{X_n\}$ be a Markov chain with $S$ as state space and stationary transition probabilities.
Let $\tau$ be a stopping time. Then for all $m\in \mathbb{N}, C \subseteq S^m, A \in \mathcal{F}_\tau,
i \in S$, if $P(A \bigcap \{X_\tau = i\}) > 0$, then it holds:

\begin{equation}
P\left((X_{\tau + 1}, \ldots, X_{\tau + m}) \in C | A, X_{\tau} = i\right) =
P\left((X_{\tau + 1}, \ldots, X_{\tau + m}) \in C | X_{\tau} = i\right)
\label{ec:strong-mc}
\end{equation}
#+end_proposition

#+begin_proof
Let us start taking $m = 1, C = \{j\}$. Then, intersecting over $\{\tau = n\}$, we have:

\begin{align*}
P\left(X_{\tau + 1} = j, A, X_{\tau} = i\right) &=
\sum_{n = 0}^{\infty} P\left(\tau = n, X_{n + 1}, A, X_n = i\right) = \\
&= \sum_{n = 0}^{\infty} P\left(\tau = n, A, X_n = i\right) \cdot P\left(X_{n + 1} = j | \tau = n, A, X_n = i\right) = \\
&= \sum_{n = 0}^{\infty} P\left(\tau = n, A, X_n = i\right) \cdot P\left(X_{n + 1} = j | X_n = i\right) = \\
&= p_{i,j} \sum_{n = 0}^{\infty} P\left(\tau = n, A, X_n = i\right) = p_{i, j} P\left(A, X_{\tau} = i\right)
\end{align*}

where we have used $A\bigcap \{\tau = n\} \in \mathcal{F}_n$ to simplify
$P\left(X_{n + 1} = j | \tau = n, A, X_n = i\right)$
as $P\left(X_{n + 1} = j | X_n = i\right)$. Therefore, using the proven equality we can show:

\[
P\left(X_{\tau + 1} = j | A, X_\tau = i\right) = \frac{P\left(X_{\tau + 1} = j, A, X_{\tau} = i\right)}{P(A, X_{\tau} = i)} = p_{i, j}
\]

Applying several times this argument, we have:

\begin{align*}
& P\left(X_{\tau + 1} = i_1, X_{\tau + 2} = i_2, \ldots, X_{\tau + m} = i_m | A, X_{\tau} = i\right) =\\
&= P\left(X_{\tau + 1} = i_1 | A, X_{\tau} = i\right) \cdot P\left(X_{\tau + 2} = i_2 | A, X_{\tau} = i, X_{\tau + 1} = i_1\right) \cdots \\
&\cdots P\left(X_{\tau + m} = i_m | A, X_{\tau} = i, X_{\tau + 1} = i_1, \ldots X_{\tau + m - 1} = i_{m - 1}\right) = p_{i, i_1} p_{i_1, i_2} \cdots p_{i_{m - 1}, i_m}
\end{align*}

In particular, when $A$ is the whole space $\Omega$, we arrive at:

\[
P\left(X_{\tau + 1} = i_1, X_{\tau + 2} = i_2, \ldots, X_{\tau + m} = i_m | X_{\tau} = i\right) = p_{i, i_1} p_{i_1, i_2} \cdots p_{i_{m - 1}, i_m}
\]

which shows the equality:

\[
P\left(X_{\tau + 1} = i_1, \ldots, X_{\tau + m} = i_m | A, X_{\tau} = i\right) =
P\left(X_{\tau + 1} = i_1, \ldots, X_{\tau + m} = i_m | X_{\tau} = i\right)
\]

We have shown that Equation \eqref{ec:strong-mc} holds for $C = \{(i_1, \ldots, i_m)\}$. For
$C \subseteq S^m$ it would suffice summing over $(i_1, \ldots, i_m) \in C$.
#+end_proof

The previous result says us that all the Markov chains (in discrete time) have the strong Markov
property, and it can be interpreted as the Markov chain restarting each time it hits the state
$i \in S$.

** Hitting times
The probabilities $p_{i, j}^{(n)} = P_i\left(X_n = j\right)$ gives us of the probability of the
chain reaching state $j$ in the $n$ -th period, if it starts from $i$. But the state $j$ could have
been visited before. We are interested now in finding the first time that a given state is reached.

#+begin_definition
We define the hitting time for the state $j \in S$ as:

\[
\tau_j = min \{n > 0 : X_n = j \}
\]
#+end_definition

#+begin_definition

We call first hitting time probability of the state $j$, provided that the chain starts in the
state $i$, to:

\[
f_{i,j}^{(n)} = P_i\left(\tau_j = n\right) = P_i\left(X_n = j, X_m \neq j \quad \forall m = 1, \ldots, n - 1 \right)
\]

and we will define the hitting probability for $j$ starting at $i$ as $f_{i, j} := \sum_{n = 0}^{\infty} f_{i,j}^{(n)} = P_i\left(\tau_j < \infty\right)$

Note that $f_{i,j} = \probi{X_n = j \textrm{ for some } n\ge 1}$
#+end_definition

Note that we coul have $f_{i, j} < 1$ iff $P_i\left(X_n \neq j, \forall n \ge 1\right) > 0$.

#+begin_definition
We will call return time distribution for the state $i$ to $f_{i,i}^{(n)} = P_i \left(\tau_i = n\right)$,
and we will denote $f_{i,i} = P_i\left(\tau_i < \infty\right)$ (i.e. the probability of ever returning
to the start state $i$.
#+end_definition

$f_{i, j}$ can be computed using the recurrence:

\begin{equation}
f_{i,j} = \probi{X_1 = j} + \sum_{k \neq j} \probi {X_1 = k} \probi {\tau_j < \infty | X_1 = k } = p_{i,j} + \sum_{k\neq j} p_{i,k} f_{k,j}
\label{eq:fij-recurrence}
\end{equation}

solving first the system formed by the equations given by $i\neq j$ (if we have $n$ states, that would be $n \cdot (n - 1)$ equations), and then substituting the computed $f_{k,j}$ values to get $f_{j,j}$.

#+begin_lemma
If $f_{i,j} = 1$ and $p_{i,k} > 0$, then $f_{k,j} = 1$
#+end_lemma

#+begin_proof
Straightforward from Equation \eqref{eq:fij-recurrence} and the fact that $\sum_{k \in S} p_{i,k} = 1$
#+end_proof

*** Computation through absorption
Let us consider a fixed state $j$. The aim is to compute $f_{i,j}^{(n)}$ with $i\neq j$.
We could freeze the chain $\{X_n\}$ once it hits $j$ in that state, that is, taking:

\[
\tildemc = \twopartdef
             {X_n} { X_r \neq j \; \forall r < n }
             {j} {X_r = j \textrm{ for some } r  < n}
\]

That is, $j$ has been transformed into an absorbing state, such that $\tildemc$ cannot leave.

The transition matrix for $\tildemc$ is given by:

\[
\tildeprob_{i, k} = \probi{\tildex_1 = k} =
  \threepartdef
    { p_{i,k} } { i\neq j }
    { 0 }       { i = j, k \neq j }
    { 1 }       { i = j, k = j }
\]

That is, the transition matrix $\tildeP$ has the same rows as $P$, except for the $j$ -th row,
which has been substituted by $(0, \ldots, 0, \underset{j}{1}, 0, \ldots, 0)$. Therefore
we have that the $i, j$ entry for the matrix $\tildeP^n$ is:

\[
\tildep_{i,j}^{(n)} = \probi{\tildemc = j} = \probi{\tau_j \le n}
\]

and hence:

\[
f_{i,j}^{(n)} = \probi{\tau_j \le n} - \probi{\tau_j \le n - 1} = \tildep_{i, j}^{(n)} - \tildep_{i, j}^{(n - 1)}
\]

So obtaining $f_{i,j}^{(n)}$ can be done by computing $\tildeP^{n - 1}$ and $\tildeP^n$,
except for $j = i$. To compute that last probability, we could use:

\[
f_{jj}^{(n)} = \twopartdef
  { p_{jj} } { n = 1 }
  { \sum_{k \in S, k \neq j} p_{j,k} f_{k,j}^{(n - 1)} } { n \ge 2 }
\]

\begin{example}
We consider a Markov chain with states $S = \{1,2,3\}$ and the following transition matrix:

\[
  P = \left(
  \begin{array}{ccc}
  \frac{1}{3} & \frac{2}{3} &      0      \\
  \frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
      0       & \frac{3}{4} & \frac{1}{4}
  \end{array}
  \right)
\]

To compute the time it takes us to reach the state $3$, we should consider:

\[
\tildeP = \left(
  \begin{array}{ccc}
  \frac{1}{3} & \frac{2}{3} & 0 \\
  \frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
       0      &      0      &   1
  \end{array}
\right) = \frac{1}{10}
\threemat {1 & 2 & 4} {1 & -1 & 3} {1 & 0 & 0} \cdot
\threemat {1 &   &  } {  &  0 &  } {  &   & \frac{5}{6}} \cdot
\threemat {0 & 0 & 10}{3 & -4 & 1} {1 & 2 &-3}
\]

It is easy to show, calling $\alpha = \frac{5}{6}$:

\[
\tilde{P}^n = \threemat
            { \frac{2}{5} \alpha^n & \frac{4}{5} \alpha^n & 1 - \alpha^{n - 1}}
            { \frac{3}{10}\alpha^n & \frac{3}{5} \alpha^n & 1 - \frac{3}{4}\alpha^{n - 1} }
            { 0 & 0 & 1 }
\]

Therefore:

\[
P_1 \left(\tau_3 = n\right) = \tildep_{1,3}^{(n)} - \tildep_{1,3}^{(n-1)} =
1 - \alpha^{n - 1} -  (1 - \alpha^{n - 2}) = (1 - \alpha) \alpha^{n - 2} = \frac{1}{6}\alpha^{n - 2} n \ge 2
\]

\[
P_2 \left( \tau_3 = n \right) = \twopartdef { \frac{1}{4} } { n = 1 }
                                            { \tildep_{2,3}^{(n)} - \tildep_{2,3}^{(n-1)} = \frac{1}{8} \alpha^{n - 2} } { n\ge 2 }
\]

To go from $3$ to itself, we could go directly or passing by $2$ (we could not go through
$1$ since $p_{3,1} = 0$:

\[
P_3\left(\tau_3 = n\right) = \threepartdef
  { \frac{1}{4} } { n = 1 }
  { \frac{3}{16} } { n = 2 }
  { \frac{3}{32} \alpha^{n - 3} } { n \ge 3 }
\]
\end{example}

*** Computation through transition probabilities

We could ask ourselves whether it is possible to relate $f_{i,j}^{(n)}$ to $p_{i,j}^{(n)}$
directly. It holds:

\begin{align}
p_{i,j}^{(n)} &= \probi{X_n = j} = \sum_{m = 1}^{n} \probi{\tau_j = m}
\probi{X_n = j | \tau_j = = m} = \nonumber \\
&= \sum_{m = 1}^{n} \probi{\tau_j = m} \probj{X_{n-m} = j} =
\sum_{m = 1}^n f_{i, j}^{(m)} p_{j,j}^{(n - m)}
\label{eq:ps-fs-relation}
\end{align}

It is not possible to express $f_{i,j}^{(n)}$ in terms of $p_{i,j}^{(n)}$, but taking the following generating functions, we can determine a theoretical relationship between both:

\[
P_{i,j}(s) = \sum_{n = 1}^{\infty} p_{i,j}^{(n)} s^n \qquad
F_{i,j}(s) = \sum_{n = 1}^{\infty} f_{i,j}^{(n)} s^n
\]

According to $\eqref{eq:ps-fs-relation}$:

\[
P_{i,j}(s) = \sum_{n = 1}^{\infty} \sum_{m = 1}^n f_{i, j}^{(m)} p_{j,j}^{(n - m)} s^n =
\sum_{n = 1}^{\infty} \sum_{m = 1}^{n - 1} f_{i, j}^{(m)} p_{j,j}^{(n - m)} s^n +
\sum_{n = 1}^{\infty} f_{i, j}^{(n)} \underbrace{p_{j,j}^{(0)}}_{1} s^n \underset{(\ast)} = F_{i,j}(s) P_{j,j}(s) + F_{i,j}(s)
\]

where in $(\ast)$ it has been used that $\sum_{n = 1}^{\infty} \sum_{m = 1}^{\infty} p_{i, j}^{(n)} f_{i,j}^{(m)} s^{n + m}$ can be rewritten as (fixes $k$, there we can take finite $m$ positive values and $k - m$ positive):

\[
\sum_{n = 1}^{\infty} \sum_{m = 1}^{n - 1} f_{i, j}^{(m)} p_{j,j}^{(n - m)} s^n
\]

We deduce:

\begin{equation}
F_{i,j}(s) = \frac{P_{i,j}(s)}{1 + P_{j,j}(s)}
\label{eq:Fs-Ps-relation}
\end{equation}

Although this is just a theoretical relationship for the moment, we can observe, on the one hand:

\[
F_{i,j}(1) = f_{i,j} = \probi{\tau_j < \infty}
\]
and if the condition $f_{i,j} = 1$ is met:

\[
F_{i,j}'(j) = \sum_{n = 1}^{\infty} n f_{i,j}^{(n)} = \expecti{\tau_j}
\]

#+begin_definition
Taking a fixed state $j$, we can define:

1. The expected number of visited to $j$  as $P_{i,j}(1) = \sum_{n = 1}^{\infty} p_{i,j}^{(n)}$
2. The total number of visits to $j$  as $V_j = \sum_{n = 1}^{\infty} \mathbb{1}_{X_n = j}$
#+end_definition

#+begin_proposition
It holds that $\expecti{V_j} = P_{i,j}(1)$
#+end_proposition

#+begin_proof
Straightforward from $\expecti{V_j} = \sum_{n = 1}^{\infty} \expecti{\mathbb{1}_{X_n = j}} = \sum_{n = 1}^{\infty} p_{i,j}^{(n)}$
#+end_proof

*** Average arrival times

#+begin_definition
If $f_{i,j} = 1$, we define the average arrival time from $i$ to $j$ as:

\[
e_{i,j} = \expecti{\tau_j} = \sum_{n = 1}^{\infty} n f_{i,j}^{(n)}
\]
#+end_definition

#+begin_proposition
e_{i,j} = 1 + \sum_{k \neq j} p_{i, k} e_{k, j}
#+end_proposition

#+begin_proof
It holds: $f_{i,j}^{(1)} = p_{i,j}$ and $f_{i,j}^{(n)} = \sum_{k\neq j} p_{i,k} f_{k,j}^{(n-1)}$.

Therefore, and having in mind that $f_{i,j} = 1$.

\begin{align*}
e_{i,j} &= \sum_{n = 1}^{\infty} n f_{i,j}^{(n)} = p_{i,j} + \sum_{n = 2}^{\infty} n \sum_{k \neq j} p_{i,k} f_{k,j}^{(n-1)} \\
        &= p_{i,j} + \sum_{k \neq j} p_{i,k} \sum_{n = 2}^{\infty} n f_{k,j}^{(n-1)} \\
        &= p_{i,j} + \sum_{k \neq j} p_{i,k} \bigg\{ \sum_{n = 2}^{\infty} f_{k,j}^{(n-1)} + \sum_{n = 2}^{\infty} (n-1)f_{k,j}^{(n-1)} \bigg\} \\
        &= p_{i,j} + \sum_{k \neq j} p_{i,k} f_{k,j} + \sum_{k \neq j} p_{i,k} e_{k,j} \\
        &\underset{\eqref{eq:fij-recurrence}}{=} 1 + \sum_{k\neq j} p_{i,k} e_{k,j}
\end{align*}
#+end_proof

Note this last proposition can be used to compute $e_{i,j}$ with a computer.

*** Mean number of visits to a state

#+begin_proposition
If $f_{j,j} \neq 1$, we have:

\begin{equation}
  \expecti{V_j} = \frac{f_{i,j}}{1 - f_{j,j}}
  \label{eq:char-mean-number-visits}
\end{equation}

Moreover, it holds $f_{i,i} = 1 \dimplies \expecti{V_i} = \infty$.

Let us note this is a characterization of $f_{i,i} = 1$. Since $\expecti{V_i} =
\sum_{n > 0} p_{i,i}^{(n)}$, it holds:

\[
  \probi{X_n = i \textrm{ for some } n} = 1 \dimplies
  \probi{X_n = i \textrm{ for infinitely many } n} = 1
\]

\label{prop:fii-equiv}
#+end_proposition

#+begin_proof
From \eqref{eq:Fs-Ps-relation} we can deduce:

\[
P_{i,j}(1) = F_{i,j}(1) \bigg\{1 + P_{j,j}(1) \equiv \expecti{V_j}\bigg\} = \frac{f_{j,j}}{1 - f_{j,j}}
\]

If $f_{i,j} \neq 1$ then $\probi{\tau_j = \infty} > 0$ and hence $\expecti{\tau_j} = \infty$.
#+end_proof

#+begin_lemma
There is an equivalence between:

1. $\exists n >0 : p_{i,j}^{(n)} > 0$
2. $\expecti{V_j} > 0$
3. $f_{i,j} > 0$

\label{lemma:fij-equivs}
#+end_lemma

#+begin_proof
Since $\expecti{V_j} = \sum_{n = 1}^{\infty} p_{i,j}^{(n)}$, clearly i. is equivalent to ii.

ii. is equivalent to iii. can be deduced from $\expecti{V_j} = f_{i,j}(1 + \expectj{V_j})$.
#+end_proof

** Classification of states
*** Communicating classes

#+begin_definition
The state $i\in E$ communicates with $j\in S$, and we will write it like $i \rightarrow j$
iff $p_{i,j}^{(n)} > 0$ for some $n > 0$.

If $i \longrightarrow j$ and $j \longrightarrow i$ then we will say that both states communicate and
we will represent it as $i \dgoto j$.
#+end_definition

#+begin_proposition
Given $i \neq j$, it holds:

\[
i \goto j \dimplies \expecti{V_j} > 0 \dimplies f_{i,j} > 0
\]
\label{goto-characterization}
#+end_proposition

#+begin_proof
Straightforward from Lemma \eqref{lemma:fij-equivs}.
#+end_proof

#+begin_definition
State $i\in S$ is essential if $i \goto j$ always implies $j \goto i$. We denote the set
of essential states by $S^{\ast}$.
#+end_definition

#+begin_lemma
If $i\in S^{\ast}$ and $i \goto j$, then $j \in S^{\ast}$
#+end_lemma

#+begin_proof
Given $j \goto k$, then by transivity $i \goto k$. Since $i$ is essential, $k \goto i$,
which applying transitivity again from $i \goto j$, gives us $k \goto j$.
#+end_proof

#+begin_proposition
$\dgoto$ is an equivalence relationship in $S^{\ast}$, and we can decompose:

\[
S^{\ast} = C_1 \cup C_2 \cup \ldots \cup C_r
\]

where for every $i, j \in C_k$, it holds $i \dgoto j$, and if $i \in C_k, j \in C_l$ where $k \neq l$, then $i \not\goto j$ or $j \not\goto i$.

\label{prop:equiv-classes}
#+end_proposition

#+begin_proof
1. Reflexivity holds because given $i \in S^{\ast}$, then $p_{i,j} > 0$ for some $j \in S$. Because of the previous lemma $j \in S^{\ast}$. Therefore $i \goto j$ implies $j \goto i$ because of the essentiality.
2. Transitivity holds trivially.
3. Transitivity holds because if $p_{i,j}^{(n)} > 0$ and $p_{j,k}^{(m)} > 0$ then
\[
p_{i,k}^{(n + m)} \ge p_{i,j}^{(n)} \cdot p_{j,k}^{(m)} > 0
\]

The second part is trivial from the deduced equivalence property.
#+end_proof

Therefore, reordering the states of the matrix, with $P_k$ is the transition matrix for the
states in $C_k$ (which only communicate with themselves), where the last rows are formed by the
transition probabilities from the non-essential states: $Q_k$,
where those are the probabilities of going from $S - S^{\ast}$ to $C_k$ (and never returning),
and $W$ where that is the probability of the non-essential states of communicating with
themselves.

\begin{equation}
  \left(\begin{array}{cccccccccc}
  \gcell P_1    &        &            &       &        &       &            &        & \\
                & \rcell & \rcell     & \rcell&        &       &            &        & \\
                & \rcell & \rcell P_2 & \rcell&        &       &            &        & \\
                & \rcell & \rcell     & \rcell&        &       &            &        & \\
                &        &            &       & \ddots &       &            &        & \\
                &        &            &       &        &\bcell &   \bcell   & \bcell & \\
                &        &            &       &        &\bcell & \bcell P_r & \bcell & \\
                &        &            &       &        &\bcell &   \bcell   & \bcell & \\
    \rowcolor{gray!20}
      Q_1       &        &   Q_2      &       & \ldots &       &    Q_r     &        & W\\
  \end{array}\right)
\label{eq:canonic-form}
\end{equation}

Each $C_k$ is said to be an irreducible and closed sub-chain of the original transition matrix
$P$; closed because from one state from $C_k$ we can only go back to another state in the same
set; irreducible because all their states communicate among themselves. The same structure of
power matrices appears in $P^n$, $n > 2$:

\begin{equation}
  \left(\begin{array}{cccccccccc}
  \gcell P_1^{n}   &        &                  &       &        &       &                   &        & \\
                   & \rcell & \rcell           & \rcell&        &       &                   &        & \\
                   & \rcell & \rcell P_2^{n}   & \rcell&        &       &                   &        & \\
                   & \rcell & \rcell           & \rcell&        &       &                   &        & \\
                   &        &                  &       & \ddots &       &                   &        & \\
                   &        &                  &       &        &\bcell &   \bcell          & \bcell & \\
                   &        &                  &       &        &\bcell & \bcell P_r^{n}    & \bcell & \\
                   &        &                  &       &        &\bcell &   \bcell          & \bcell & \\
    \rowcolor{gray!20}
      Q_1 \cdot P_1^{n-1} & & Q_2 \cdot P_2^{n-1} &    & \ldots &       & Q_r \cdot P_r^{n} &        & W^n\\
  \end{array}\right)
\label{eq:canonic-form-pow}
\end{equation}

#+begin_definition
A Markov chain where all its states are essential and with only a communicating class is called
irreducible Markov chain.
#+end_definition

*** Recurrence and transience

#+begin_definition
An state $i\in S$ is called recurrent iff $f_{i,i} = 1$ and transient iff $f_{i,i} < 0$
#+end_definition

By Proposition \eqref{prop:fii-equiv} it is easy to show the following characterization:

#+begin_proposition
It holds:

1. $i\in S$ is recurrent iff $\probi{X_n = i \textrm{ for infinitely many } n} = 1$
2. $i\in S$ is transient iff $\probi{X_n = i \textrm{ for infinitely many } n} = 0$
\label{prop:rec-tran-characterization}
#+end_proposition

If an state is recurrent, then $\probi{V_i = \infty} = 1$ and $\expecti{V_i} = \infty$ and
$\probj{V_i = 0 \vee V_i = \infty} = 1$. If an state is transient, then $\probi{V_i < \infty}$
and $\expecti{V_i} < \infty$.

Therefore $\sum_{n = 1}^{\infty} p_{i,i}^{(n)}$ diverges when $i$ is recurrent and converges
when $i$ is transient.

#+begin_proposition
If $i$ is a recurrent state and $j$ communicates with $i$, then $f_{j,i} = f_{i,j} = f_{j,j} = 1$,
and $j$ is recurrent. Therefore, all non essential states are transient.
\label{prop:recurrent-classes}
#+end_proposition

#+begin_proof
By Proposition \eqref{goto-characterization}, $f_{i,j} > 0$. Since $i$ is recurrent $f_{ii} = 1$
and by Equation \eqref{eq:fij-recurrence}:

\[
1 = f_{i,i} = p_{i,i} + \sum_{k \neq i} p_{i,k} f_{k,i}
\]

$i \goto j$ means there exists $k_1, k_2, \ldots k_r$ verifying:

\[
p{i, k_1} \cdot p_{k_1, k_2} \cdots p_{k_{r}, k_j} > 0
\]

Since $\sum_{k} p_{k,k} = 1$, and $0 \le f_{k,i} \le 1$, then it must hold $f_{k,i} = 1$ for all
$k$ such that $p_{i,k} > 0$. In particular $f_{k_1,i} = 1$, and we deduce:

\[
1 = f_{k_1,i} = p_{k_1,k_1} + \sum_{k\neq k_1} p_{k_1,k} f_{k,i}
\]

We get $1 = f_{k_2, i}$ by an analogous argument, and we can recursively proceed until we
arrive at $f_{j,i} = 1$.

Since we have proven $f_{i,i} > 0$, $f_{i,j} > 0$, $f_{j,i} = 1$ then by Lemma \eqref{lemma:fij-equivs}
we deduce there exist $u, v, n$: $p_{i,j}^{(u)} > 0$, $p_{j,i}^{(v)}, $p_{i,i}^{(n)} > 0$.
#+end_proof

So for every $n \ge u + v$ we deduce:

\[
p_{j,j}^{(n)} \ge p_{j,i}^{(v)} p_{i,i}^{(n - u - v)} p_{i,j}^{(u)}
\]

Since $f_{i,i} = 1$, then $\sum_{n = 1}^{\infty} p_{i,i}^{(n)} = \infty$ by Proposition \eqref{prop:fii-equiv},
and therefore $\sum_{n = u + v}^{\infty} p_{i,i}^{(n)} = \infty$

\begin{align*}
\sum_{n = 1}^{\infty} p_{j,j}^{(n)} \ge \sum_{n = u + v}^{\infty} p_{j,j}^{(n)} \ge
p_{j,i}^{(v)} p_{i,j}^{(u)} \sum_{n = u + v}^{\infty} p_{i,i}^{(n)} = \infty
\end{align*}

Again from Proposition \eqref{prop:fii-equiv} we deduce $f_{j,j} = 1$. We have proved $j$ is
recurrent, but also that $j \goto i$, and we can deduce $f_{i,j} = 1$ by the same procedure
we started this demonstration with.
#+end_proof

#+begin_corollary
If $i,j \in S$ are in the same communicating class they are both either recurrent or transient.
\label{corollary:comm-classes}
#+end_corollary

#+begin_definition
We will say a communicating class is recurrent (resp. transient) iff one of its states (iff
all, by the previous corollary) are recurrent (resp. transient).
#+end_definition

#+begin_proposition
All the closed irreducible subchains of a Markov chain (with finite space state S) are recurrent,
and every Markov chain has at least one subchain of such type. A state is transient iff it is
non-essential.
\label{prop:one-recurrent-class}
#+end_proposition

#+begin_proof
Let us fix some $i \in S$. We have

\[
\sum_{j \in S} \expecti{V_j} = \sum_{j \in S} \sum_{n = 1}^{\infty} p_{i,j}^{(n)} =
\sum_{n = 1}^{\infty} \sum_{j \in S} p_{i,j}^{(n)} = \sum_{n = 1}^{\infty} 1 = \infty
\]

And therefore, it exits at least one $\expecti{V_j} = \infty \equiv f_{j,j} = 1$, so the subchain which
contains $j$ is recurrent. The same argument can be applied to the other subchains. Therefore, if we have
a chain and a certain state $j$ in that chain, $f_{j,j} = 1$ and $j$ cannot be transient. In other words,
if $j$ is transient, $j$ is non-essential.
#+end_proof

#+begin_corollary
In a Markov chain with finitely many states, a communicating class is recurrent iff it is closed.
\label{corollary:comm-recurrence}
#+end_corollary

#+begin_proof
If we have a non closed class $C$, there exist $i \in C, j \in S - C$ such that $p_{i,j} > 0$ and
$p_{j, i} = 0$. Therefore $f_{j,i} = 0$ by \eqref{goto-characterization}.

Therefore:

\begin{align*}
f_{i,i} &= p_{i,i} + \sum_{k \neq i} p_{i, k} f_{k,i} = \\
        &= p_{i,i} + \sum_{k \neq i, j} p_{i, k} f_{k, i} < \\
        &< \sum_{k \neq j} p_{i, k} \underset{p_{i,j} > 0}{<} 1
\end{align*}

and the class would not be recurrent.

Reciprocally, Proposition \eqref{prop:one-recurrent-class} gives us the result we want.
#+end_proof

We are going to make the classification among the recurrent states even finer.

#+begin_definition
We say that recurrent state $i \in S$ is:
1. positive recurrent iff $e_{i,i} < \infty$.
2. null recurrent iff $e_{i,i} = \infty$.
#+end_definition

#+begin_lemma
A state $i$ is recurrent positive iff:

\[
lim_{s\nearrow 1} \frac{1 - F_{i,i}(s)}{1 - s} < \infty \dimplies lim_{s\nearrow 1} (1 - s) \bigg\{1 + P_{i,i}(s)\bigg\} > 0
\]

\label{lemma:positive-rec-characterization}
#+end_lemma

#+begin_proof
Let us observe that for the limit exists it must be $\lim_{s\nearrow 1} F_{i,i}(s) = f_{i,i} = 1$ iff $i$ is recurrent.

\[
\lim_{s\nearrow 1} \frac{1 - F_{i,i}(s)}{1 - s} = \frac{1}{1 - s} \sum_{n = 1}^{\infty} (1 - s^n) f_{i,i}^{(n)} =
\sum_{n = 1}^{\infty} (1 + s + \ldots + s^{n-1}) f_{i,i}^{(n)}
\]

which tends to $\sum_{n = 1}^\infty n f_{i,i}^{(n)} = e_{i,i}$.

The second part is deduced from Equation \eqref{eq:Fs-Ps-relation}.
#+end_proof

#+begin_proposition
If $i,j$ are communicating recurrent states, then they are both positive recurrent or null recurrent states.
\label{prop:all-positive-all-null}
#+end_proposition

#+begin_proof
By the procedure used in the proof of Proposition \eqref{prop:recurrent-classes}, we have:

\[
p_{j,j}^{(n)} \ge p_{j,i}^{(v)} \cdot p_{i,i}^{(n-u-v)} \cdot p_{i,j}^{(u)} > 0 \forall n > u + v
\]

Therefore:

\begin{align*}
(1-s) \bigg\{1 + P_{j,j}(s)\bigg\} = (1-s) \bigg\{1 + \sum_{n = 1}^{\infty} p_{j,j}^{(n)} s^n\bigg\} = \\
(1-s) \bigg\{1 + \sum_{n = 1}^{u + v} p_{j,j}^{(n)} s^n + \sum_{n = u + v + 1}^{\infty} p_{j,j}^{(n)} s^n \bigg\} \ge \\
(1-s) \bigg\{1 + \sum_{n = 1}^{u + v} p_{j,j}^{(n)} s^n + s^{u + v} p_{i,j}^{(u)} p_{j,i}^{(v)} \sum_{n = 1}^{\infty} p_{i,i}^{(n)} s^n \ge \\
(1-s) \bigg\{1 + s^{u + v} p_{i,j}^{(u)} p_{j,i}^{(v)} P_{i,i}(s) \bigg\} =\\
(1-s) \bigg\{1 + P_{i,i}(s)\bigg\}s^{u + v} p_{i,j}^{(u)} + (1-s) \bigg\{1 - s^{u + v} p_{i,j}^{(u)} p_{j,i}^{(v)} \bigg\}
\end{align*}

Hence:

\[
\lim_{s\nearrow 1} (1-s) \bigg\{1 + P_{j,j}(s)\bigg\} \ge p_{i,j}^{(u)} p_{j,i}^{(v)} \lim_{s\nearrow 1} (1-s) \bigg\{1 + P_{i,i}(s)\bigg\}
\]

And exchanging the roles of $i$ and $j$, we deduce $i$ is positive recurrent iff $j$ is.
#+end_proof

#+begin_proposition
Every irreducible Markov chain with a finite number of states is positive recurrent
#+end_proposition

#+begin_proof
Let us fix a $s \in [0,1[$. We have, using that $\sum_{n = 1}^{\infty} = \frac{s}{1 - s}$:

\[
\sum_{j \in S} (1-s) F_{i,j}(s) \bigg\{1 + P_{j,j}(s)\bigg\} \underset{\textrm{\eqref{eq:Fs-Ps-relation}}}{=}
(1 - s) \sum_{j \in S} P_{i,j}(s) = (1-s) \sum_{n = 1}^{\infty} s^n \underbrace{\sum_{j\in S} p_{i,j}^{(n)}}_{1} = s
\]

Taking $\lim_{s \nearrow 1}$ we have $\sum_{j \in S} \lim_{s \nearrow  1} (1-s) F_{i,j}(s) = 1$

Therefore, it must exist $j \in S$ with $\lim_{s \nearrow  1} (1-s) F_{i,j}(s) > 0$. By Lemma \eqref{lemma:positive-rec-characterization},
we deduce $j$ is positive recurrent, therefore recurrent. By Proposition \eqref{prop:recurrent-classes} all states
communicating with $j$ are also recurrent, and by Proposition \eqref{prop:all-positive-all-null}, all the states of
the chain are positive recurrent.
#+end_proof

*** Absorption

Although the concept of absorbing state has arisen before, we give a formal definition here:

#+begin_definition
A state $i\in S$ is absorbing iff $p_{i,i} = 1$.
#+end_definition

Let us observe the matrix \eqref{eq:canonic-form}. This matrix poses some interesting questions for
the non-essential states. Let us take a non-essential state, namely $i$. Then it is likely that it will go to a
state in $S^{\ast}$ at some point, and never comes back.

#+begin_definition
We call the absorbing time to the following random variable:

\[
\tau^{\ast} = \min \{n > 0: X_n \in S^{\ast}\}
\]

If $\tau^{\ast} < \infty$, we can define the entry absorbing state as $X_{\tau^{\ast}}$.

We define:

1. The absorbing probability as $f_{i}^{\ast} = \probi{\tau^{\ast} < \infty}$.
2. Taking $j\in S^{\ast}$, we can define the probability of being absorbed by $j$ as
$f_{i,j}^{\ast} = \probi{\tau^{\ast} < \infty, X_{\tau^{\ast}} = j}$.
3. Since we can decompse $S^{\ast} = C_1 \cup C_2 \cup \ldots \cup C_r$ by the Proposition
\eqref{prop:equiv-classes}. Given $C = C_i$, we can define the probability of the state $i$
being absorbed by the subchain $C$ as:

\[
f_{i,C}^{\ast} = \probi{\tau^{\ast} < \infty, X_{\tau^{\ast}} \in C}
\]
#+end_definition

#+begin_proposition
The following relations hold for each $i \not\in S^{\ast}$:

1. $f_i^{\ast} = \sum_{j \in S^{\ast}} p_{i,j} + \sum_{j \not\in S^{\ast}} p_{i,j} f_{j}^{\ast}$
2. $f_{i,j} = p_{i,j} + \sum_{k \not\in S^{\ast}} p_{i,j} f_{k,j}^{\ast}$
2. $f_{i,C}^{\ast} = \sum_{j \in C} p_{i,j} + \sum_{j \not\in S^{\ast}} p_{i,j} f_{j}^{\ast}$
#+end_proposition

It is obvious that $f_{i,C}^{\ast} = \sum_{j \in C} f_{i,j}^{\ast}$. Once the chain enters the
subchain, $C$, it does so with the initial probabilities
$(f_{i,j}^{\ast} / f_{i,C}^{\ast})_{j \in C}$.

It we call $V = \bigg(\expecti{V_j}\bigg)_{i,j \not\in S^{\ast}}$, we have, taking a look at
the matrix that \eqref{eq:canonic-form-pow} that $V = \sum_{n = 1}^{\infty} W^n$.

Also, we can define $Q = (Q_1, \ldots, Q_r)$ the matrix of transition probabilities from
$S - S^{\ast}$ to $S^{\ast}$, and $q = Q \cdot \left(\begin{array}{c} 1 \\ 1 \\ \vdots \\ 1 \end{array}\right)$ (the row-wise sum of $Q$). And we will call
$F^{\ast} = (f_{i,j}^{\ast})_{i \in S - S^{\ast}, j \in S^{\ast}}$,
$f^{\ast} = (f_{i}^{\ast})_{i \in S - S^{\ast}}$.

With those notations, we can establish the following proposition:

#+begin_proposition
The following relations hold:

\begin{align}
F^{\ast} &= (I + V)Q \label{prop:Fast-matrix}\\
f^{\ast} &= (I + V)q \label{prop:fast-matrix}
\end{align}

The distribution for $\tau^{\ast}$ is $\probi{\tau^{\ast} = n} = B^{n - 1}q, \quad n \in \mathbb{N}$
\label{prop:fast-matrices}
#+end_proposition

#+begin_proof
Let us fix $i\in S - S^{\ast}, j \in S^{\ast}$. We have:

\begin{align}
\probi{\tau^{\ast} = n + 1, X_{\tau^{\ast}} = j} = \sum_{k \in S - S^{\ast}} \probi{X_n = k, X_{n + 1} = j} = \sum_{k \in S - S^{\ast}} p_{i,k}^{(n)} p_{k,j}
\label{eq:tau-ast-nat}
\end{align}

Using that for $i$ to get absorbed by $j$ in time $n + 1$, one cannot go through a state in $S^{\ast}$ (otherwise absorption would happen). Also, using $p_{i,j}^{(0)} = \delta_{i,j}$, we have:

\begin{align}
\probi{\tau^{\ast} < \infty, X_{\tau^{\ast}} = j} = \sum_{n = 0}^{\infty} \sum_{k \in S - S^{\ast}} p_{i,k}^{(n)} p_{k,j} = \sum_{k \in S - S^{\ast}} \bigg(\delta_{i,k} + \expecti{V_k}\bigg) p_{k,j}
\label{eq:tau-ast-inf}
\end{align}

Let us note that this last equality is \eqref{prop:Fast-matrix} expressed matrix-wise.

To get \eqref{prop:fast-matrix}, it suffices to sum in $j \in S^{\ast}$ in Equation \eqref{eq:tau-ast-inf}.

Summing in $j \in S^{\ast}$ in \eqref{eq:tau-ast-nat} we arrive at:

\[
\probi{\tau^{\ast} = n + 1} \sum_{k \in S - S^{\ast}} p_{i,k}^{(n)} q_k = (B^n q)_i
\]
#+end_proof

#+begin_definition
The matrix $N = (I - W)^{-1}$ is called fundamental matrix for $P$. The entry $n_{ij}$ represents the
number of times the process is in trasient state $_{j}$ if it is started in also transient state $s_{i}$,
(counting the initial state as $1$ step)
#+end_definition

#+begin_definition
Let $t_i$ be the number of steps before the chain is absorbed, given that the chain starts in state $s_i$.
Then $t = N\cdot \onecol$
#+end_definition

#+begin_proposition
If $S - S^{\ast}$ is finite, the following relation holds:

\begin{eqnarray}
F^{\ast} = (I - W)^{-1} Q \label{eq:char-absorption-probs}\\
f^{\ast} = 1 \nonumber
\end{eqnarray}

where the last equation tells us the absorption would take place almost surely.
#+end_proposition

#+begin_proof
It holds:
\[
(I + V)(I - W) = (I + W + W^2 + \ldots )(I - W) = I
\]
and therefore $(I + V) = (I - W)^{-1}$ (if we had not had finiteness we could not assure
uniqueness for the inverse).

To check the other part, since the rows of the transition matrix from $S - S^{\ast}$ sum one,

\[
Q1_{S^{\ast}} + W 1_{S - S^{\ast}} = 1_{S - S^{\ast}} \dimplies Q1_{S^{\ast}} = (I - W) 1_{S - S^{\ast}}
\]

Therefore:

\[
f^{\ast} = F^{\ast} 1 = (I - W)^{-1} \cdot Q 1 = (I - B)^{-1}(I - B) 1_{S - S^{\ast}} = 1_{S - S^{\ast}}
\]

#+end_proof

#+begin_proposition
$F^{\ast}$ is the unique solution, with all its elements in $[0,1]$, of the system $(I-B)X = a$ iff
absorption in $E^{\ast}$ is sure from every state in $E - E^{\ast}$.
#+end_proposition

#+begin_proof
Let us fix some $i \in E - E^{\ast}$, and assume $A >= B$ as a matrix iff:

\[
\sum_{i, j}{a_{i,j}} < \sum_{i, j}{b_{i,j}}
\]

From $(I - W) X = Q$ with $X$ with all its entries in $[0,1]$, we deduce:

\begin{align*}
X = Q + XW \ge Q \underset{X = Q + XW}{\implies} X \ge Q + QW \underset{X = Q + XW}{\implies} \\
X \ge Q + QW + AB^2 \implies \ldots \implies X \ge (I + V)Q = F^{\ast}
\end{align*}

Therefore $F^{\ast}$ is the minimum solution with entries in $[0,1]$ to the equation $(I - W)X = W$
and analogously $f^{\ast}$ is the minimum solution to $(I-B)x = a$.

On the one hand, if there exists solutions to $(I - W)X = Q$ with their elements on $[0,1]$ and not equal to $F^{\ast}$,
then $x = X 1$ would be solution to $(I - W)x = q$ and $f^{\ast} < x$ cannot be $1$.

On the other hand, if absorption is not sure, $x_i = 1 - f_i^{\ast}$ verify $x = Wx$. Therefore, $x = Bx$
has some non null solution, with $0 \le x \le 1$ and $X = F^{\ast} + x1$ is a solution for
$(I - W)X = Q$ whose elements are in $[0,1]$, since:

\[
f_{i,j}^{\ast} + x_{i} = \probi{\tau^{\ast} < \infty, X_{\tau^{\infty}} = j} + \probi{\tau^{\ast} = \infty} \le \probi{\tau^{\ast} < \infty} + \probi{\tau^{\ast} = \infty} = 1
\]

#+end_proof

** Ergodic Markov chains
#+begin_definition
We say that a Markov chain is ergodic iff all states communicate with each other
#+end_definition

#+begin_definition
We say that a Markov chain is regular iff there exists some power of its transition matrix with
all positive elements
#+end_definition

#+begin_proposition
Every regular Markov chain is also ergodic. The opposite does not hold.
#+end_proposition

#+begin_proof
If the Markov chain is regular, then there exists $n\in \naturals$ such that for every $i, j\in S$, then
$p_{i,j}^{(n)} > 0$ and therefore $i \dgoto j$, with $i,j$ arbitrary states.

For the counterexample, it suffices to take the Markov chain given by the transition matrix
$P = \left(\begin{array}{cc} 0 & 1 \\ 1 & 0 \end{array}\right)$.
#+end_proof

#+begin_lemma
If $P_{m\times m}$ is an ergodic transition matrix for a Markov chain with states $\{1, \ldots, m\}$, then
it is possible to reach each state $j$ from $i$ in $m-1$ steps at most.

\label{lemma:reachability}
#+end_lemma

#+begin_proof
The proof is based on graphs. Given a path of positive transition probabilities from
$i$ to $j$, $i, k_1, k_2, \ldots, k_n = j \in S$, since $S$ is finite, if there were two states repeated,
$k_l, k_p$ with $p > l$, we could delete the subpath $k_l, k_{l + 1}, \ldots $k_{p-1}$ and we would still
have a path of states that would take us to $j$. Therefore we cannot have repeated elements in that path,
and $n$ can be at most $m - 1$.
#+end_proof

#+begin_lemma
Given $P_{m\times m}$ regular, then $P^n$ is regular for every $n\in \naturals$
\label{lemma:power-regular}
#+end_lemma

#+begin_proof
$P$ is regular, so all of its entries are positive. Therefore $P\cdot P$ gives a matrix of positive entries,
and we can do induction on the number of products.
#+end_proof

The last lemma makes possible to define:

#+begin_definition
Given $P$ a regular matrix, we will call $\gamma(P) = \min\{n \in \naturals: P^n > 0\}$
#+end_definition

#+begin_theorem
Let $P_{m \times m}$ be a regular matrix. Let $L_i = \{k_1^{(i)}, k_2^{(i)}, \ldots \}$ the set of lengths
of all directed paths of $G(P)$ which start and end at the node $i$, with $i = 1, \ldots, n$. Let
$g_i = gcd(L_i)$ (which we shown in previous sections that it was well defined for infinite sets). Then
$P$ is regular iff $g_i = 1 \forall i = 1, \ldots, n$.

\label{th:regular-characterization}
#+end_theorem

#+begin_proof
We are going to prove only that if $P$ is regular, then $g_i = 1 \forall i = 1, \ldots, n$, since this part is
will be used by another result later on. But the counterpart is also true, although it lies out of the
scope of this work.

Since $A$ is primitive, there exists $m \in \naturals$ such that $A^m > 0$. Because of Lemma
\eqref{lemma:power-regular} we have $A^{m + p} > 0$ for all $p \ge 0$. Therefore there exists a path of
(large enough) prime length in each $L_i$ and therefore $g_i = 1$.
#+end_proof

#+begin_theorem
If $P_{m\times m}$ is regular and the shortest cycle in $G(P)$ has length $s$, then
$\gamma(P) \le m + s(m - 2)$, and $A^{m + s(m - 2)} > 0$.

\label{th:regular-gamma-bound}
#+end_theorem

#+begin_proof
We may assume, w.l.o.g. that we have a shortest cycle is formed by the sequence with no repeated elements
given by $[1, \ldots, s]$ (since in a stochastic matrix we
can reorder the states and the related properties of reachability such as states classification, powers, etc
translate correctly from the original to the reordered matrix)

Since $P$ is ergodic (regular $implies$ ergodic), it exists a path from every node $i$ to every other node
$j$, and that path has at most length $m-1$ by Lemma \eqref{lemma:reachability}.

Let us split $P^{m + s(m - 2)} = P^{m - s}(P^s)^{m -1}$ and we partition:

\[
P^{m - s} =
\left(\begin{array}{c|c}
  \underbrace{X_{11}}_{s\times s} & \underbrace{X_{12}}_{s \times (n - s)} \\
  \hline
  \underbrace{X_{21}}_{(n - s)\times s} & \underbrace{X_{22}}_{(n - s) \times (n - s)}
\end{array}\right)
\qquad
(P^s)^{m-1} = \left(\begin{array}{c|c}
  \underbrace{Y_{11}}_{s\times s} & \underbrace{Y_{12}}_{s \times (n - s)} \\
  \hline
  \underbrace{Y_{21}}_{(n - s)\times s} & \underbrace{Y_{22}}_{(n - s) \times (n - s)}
\end{array}\right)
\]

Since $[1, \ldots, s]$ form a cycle we can go from every $i$ to every other $j$ with $i,j \in \{1, \ldots, s\}$.
Therefore there must be at least one positive element in each row of $X_{11}$. From the fact that the matrix
is irreducible, in $r \le m - s$ steps we must be able to go from one state in $i \in \{s + 1, \ldots, m\}$ to another
state $j \in \{1, \ldots s\}$ and stay inside the path $j, \ldots s, 1, 2, \ldots s$ for the remaining
$m - s - r$ steps. Therefore every row in $X_{21}$ must contain one strictly positive value.

Since $[1, \ldots, s]$ form a cycle in $G(P)$ we have a self cycles $[1], [2], \ldots, [s]$ in G(P^s)$.
$P$ is regular and by Lemma \eqref{lemma:power-regular} $P^s$ is also regular and thus ergodic. Therefore,
there must exist a path $[i, \ldots, j]$ of length $m - 1$ at most, with $i,j \in \{1, \ldots, m\}$. And
looping enough steps in the cycle $[i]$ before, we have that $Y_{11} > 0$ and $Y_{12} > 0$.

To end the proof, it suffices to write:

\[
P^{m - s} \cdot (P^s)^{m-1} =
  \left(\begin{array}{c|c}
  X_{11} & X_{12} \\
  \hline
  X_{21} & X_{22}
  \end{array}\right)
\cdot
  \left(\begin{array}{c|c}
  Y_{11} & Y_{12} \\
  \hline
  Y_{21} & Y_{22}
  \end{array}\right)
\ge
  \left(\begin{array}{c|c}
  X_{11} \cdot Y_{11} & X_{11} \cdot Y_{12} \\
  \hline
  X_{21} \cdot Y_{11} & X_{21} \cdot Y_{12}
  \end{array}\right)
\]

and using the fact that if we have a matrix $A > 0, x \ge 0, x\neq 0$, then $Ax > 0$ (and analogously by
row-wise products instead of column-wise), we have shown $P^{m + s(m - 2)} > 0$.
#+end_proof


#+begin_corollary
$P_{m \times m}$ is regular iff $P^{m^2 - 2m + 2} > 0$
#+end_corollary

#+begin_proof
One implication is clear by the definition of regular.

The counterpart: if $P$ is regular, then there are cycles. If the shortest one had length $n$
(all have length lower or equal than $n$ because we need at most $n-1$ steps to go from a state to
antoher), then by Theorem \eqref{th:regular-characterization} $g_i = n$ for all $i = 1, \ldots, m$
and matrix could not be regular. Then the shortest cycle must have length $s \le n - 1$, and by
\eqref{th:regular-gamma-bound} we would have:

\[\gamma(P) \le n + s(n - 2) \le n + (n - 1)(n - 2) = n^2 - 2n + 2 \]
#+end_proof

#+begin_lemma
If $P_{m \times m}$ is ergodic and it has some $1 \le d \le m$ diagonal entries strictly positive, then
$P$ is regular and $\gamma(P) \le 2m - d - 1$
#+end_lemma

#+begin_proof
$A$ must be primitive, since there would exist at least one cycle $[i]$, and the matrix is ergodic, so we
can have as many loops as necessary and then a path $[i, \ldots j]$ (of length lower or equal than $m$) or
$[j, \ldots i]$ and appropriate number of loops in $[i]$, or junction of paths $[j, \ldots, i]$ plus
enough loops in $[i]$, plus the path $[i, \ldots, k]$. To sum up, we can prove $A^{2m} > 0$.

Now we know $A$ must have $d$ cycles with length one. Let us suppose, w.l.o.g., that they are at
$1, \ldots, d$ states. We can partition:

\[
P^{m - d} =
\left(\begin{array}{c|c}
  \underbrace{X_{11}}_{d\times d} & \underbrace{X_{12}}_{s \times (m - d)} \\
  \hline
  \underbrace{X_{21}}_{(m - d)\times d} & \underbrace{X_{22}}_{(m - d) \times (m -d)}
\end{array}\right)
\qquad
(P^1)^{m - 1} = \left(\begin{array}{c|c}
  \underbrace{Y_{11}}_{d\times d} & \underbrace{Y_{12}}_{s \times (m - d)} \\
  \hline
  \underbrace{Y_{21}}_{(m - d)\times d} & \underbrace{Y_{22}}_{(m - d) \times (m -d)}
\end{array}\right)
\]

and by a similar argument to the one in the demonstration of \eqref{th:regular-gamma-bound} we can show
that $X_{11}$ and $X_{21}$ contain at least one positive entry per row and that $Y_{11} > 0$ and
$Y_{12} > 0$. For this last part it suffices to use the argument that we can loop in $[i] for $n - 1 - r_i$
times and then in $r_i \le n - 1$ steps we can reach from $i \in \{1, \ldots, d\}$ to
$j \in \{1, \ldots, m\}$.
#+end_proof

#+begin_lemma
If $P$ is an ergodic stochastic matrix with $p_ii > 0$ for all $i \in \{1, \ldots, m\}$ then is a regular
stochastic matrix.
\label{lemma:ergodic-pos-diagonal}
#+end_lemma

#+begin_proof
Fixed $i,j$ and because of the previous lemma \eqref{lemma:reachability} we know that there exists a
power $P^{m_{ij}}$ such that $p_{ij} > 0$. It suffices to prove that $\widetilde{P} = \prod_{i = 1}^m \prod{j = 1}^m P^{m_{ij}}$
is a matrix with all possitive entries.

If $P^{m_ij}$ has $\{p_{lk}\}$ positive entries, it is inmediate (since the diagonal of $P$ is non null), that
$P\cdot P^{m_ij}$ has all those positive entries (at least). Using that argument recursively, we prove that
$\widetilde{P}$ has all of its entries positive.
#+end_proof

#+begin_definition
Given a matrix $P$, a row vector $r = \left(r_1, \ldots, r_n\right)$ is called a fixed row vector
iff $rP = r$ (it is a left eigenvector for the matrix). Analogously, a column vector $c = \left(\begin{array}{c} r_1 \\ \vdots \\ r_n\end{array}\right)$
is called a fixed column vector iff $Pc = c$ (it is a right eigenvector for the matrix).

We will note $\onecol$ to the column vector $\left(\begin{array}{c} 1 \\ \vdots \\ 1 \end{array}\right)$
#+end_definition

#+begin_definition
Given a stochastic matrix $P$, we call steady state to a probability vector $v$ such that $vP = v$
#+end_definition

#+begin_lemma
Let $P_{m\times m}$ be a transition matrix with all its entries positve. Let us fix a probability column
vector $y$ and name $M_0 = \max y$, $m_0 = \min y$, $M_1 = \max Py$ and $m_1 = \min Py$. Hence, there exists
$k\in [0,1[$

\[
M_1 - m_1 \le k (M_0 - m_0)
\]

\label{lemma:bound-pos-matrix}
#+end_lemma

#+begin_proof
Let us call $d = \min\{p_{ij}: i,j\in \{1, \ldots, m\}\}$. If $m = 1$, it suffices to
take $c = 0$. If $m > 1$, then $d \le \frac{1}{2}$ (no entry is null). Since $Py$ can be
thought as a weighted average of the entries in $y$, the maximum value we can reach is bounded:

\[ M_1 \le d m_0 + (1 - d)M_0 \]

and likewise for the smallest possible one:

\[ m_1 \ge d M_0 + (1 - d)m_0 \]

Hence:

\[ M_1 - m_1 \le d m_0 + (1 - d)M_0 - dM_0 - (1 - d)m_0 = (1 - 2d)(M_0 - m_0) \]

and we can take $c = 1 - 2d$.
#+end_proof

#+begin_theorem
**Fundamental limit theorem for regular Markov chains**

Let $P_{m\times m}$ be the transition matrix of a regular Markov chain. Then $\{P^{n}\} \rightarrow W$ where
$w \in \posreals$ and $W = \left(\begin{array}{c} w \\ \vdots \\ w \end{array}\right)$. Moreover, $w$ is a
probability vector with all its components strictly positive.

If $P_{m\times m}$ is a regular transition matrix and
$W = \left(\begin{array}{c} w \\ \vdots \\ w \end{array}\right) = \ \lim_{n} W^n$, then it holds:

1. $wP = w$ and every other fixed row vector $r$ holds $r = \lambda w$ for some $\lambda \in \mathbb{R}$
2. $P\onecol = \onecol$ and every fixed column vector $c$ holds
$c = \lambda \onecol$ for some $\lambda \in \mathbb{R}$

Therefore an stochastic regular matrix has a unique steady state.
\label{th:steady-regular}
#+end_theorem

#+begin_proof
/First part/

Let us first prove the theorem for a matrix $P$ with all its elements strictly positive. Given $y$ a probability column
vector, let us call:

\begin{equation}
M_n = \max (P^n y) \ge (P^n y)_{ij} \ge m_n = \min (P^n y)
\label{eq:mns-sandwich}
\end{equation}

Since each time we multiply a vector by $P$ we do a weighted average, its maximum element would be lower
than the previous maximum, and the contrary will happen for its minimum. Hence:

\begin{equation}
  m_0 \le m_1 \le \ldots m_n \le M_n \le \ldots M_1 \ldots M_0
\label{eq:ms-sandwich}
\end{equation}

In other words, $\{m_n\}$ and $\{M_n\}$ are monotonic sequences of numbers, and their limit exits:

\[
  \{m_n\} \rightarrow m \qquad \{M_n\} \rightarrow M
\]

By the Lemma \eqref{lemma:bound-pos-matrix}

\[
  M_n - m_n \le k (M_{n-1} - m_{n-1}) \le \ldots \le k^n M_0 - m_0
\]

with $k\in [0,1[$. Therefore $v := \lim M_n = \lim m_n$, and \eqref{eq:mns-sandwich} let us observe that for
every probability vector $y$ it holds
$\{P^n y \} \rightarrow \left(\begin{array}{c} m \\ m \\ \vdots \\ m \end{array}\right)$.

Let us observe that $v$ depends on $y$. If we take the column probability vector with $1$ only in the $j$ -th
position, namely $e_j$, then calling $w_j$ the limit that depends on $e_j$, we have that:

\[
  \{P^n e_j \} \rightarrow v_j
\]

That is, the $j$ -th column of $P^n$ tends to a constant value $v_j$. In other words,
$(P^n)_i \rightarrow w = (v_1, \ldots v_m)$, and
$P^n \rightarrow W = \left\{\begin{array}{c} w \\ \vdots \\ w \end{array}\right\}$.

Since $m_0 \ge 0$ and the sequence is strictly increasing, $v > 0$ and therefore all the entries for $w$ are
strictly positive.

To extend the demonstration to regular matrices $P$, it suffices to remember that by their definition there
must exist a $l > 0$ such that $P^l$ has only positive entries. Hence, $\{m_{ln}\}$ and $\{M_{ln}\}$
converge to the same limit. Equations \eqref{eq:mns-sandwich} and \eqref{eq:ms-sandwich} were obtained
for regular stochastic matrices, and therefore for $n$ sufficiently large we can find a value $k$ such that
$m_{lk} \le m_p \le M_p \le M_{lk} \forall p \ge n$. And therefore we can assure there is convergence of
$\{m_n\}$ and $\{M_n\}$ and use the rest of the demonstration for the case of positive entries.

Since $Py$ is a probability vector if $y$ is a probability vector, $\sum_{j=1}^m (P^n y)j = 1$ and taking
limits in $n$, $\sum_{j=1}^m w_j = 1$ and $w \ge 0$ implies that it must be a probability vector.
/Second part/

1. If $P^{n + 1} = P^{n} \cdot P \goto WP = W$, then we deduce $wP = w$ row-wise. For other fixed row vector $r$
it holds $r = rW = \big(\sum_{i = 1}^m r_i \big) W$

2. Since $c = Pc = P^2 c = \ldots P^n c$, taking limits $c = Wc$, and therefore
\[
  c = \left(\begin{array}{c} w \cdot c \\ \vdots \\ w \cdot c \end{array}\right) = \onecol \cdot w \cdot c
\]
#+end_proof

As consequence of this theorem, since each element $(i,j)$ of $P^n$ is $p_{i,j}^{(n)}$ and each row tends
to the same value $w$, the probability of starting at state $i$ and being in $j$ after $n$ steps is
independent of the starting state $i$. \eqref{th:steady-regular} has an analogous theorem for ergodic
Markov chains. We will need some previous lemma to show that.


#+begin_lemma
Given $P$ an ergodic stochastic matrix, then $P$ and $\frac{1}{2}(I + P)$ is regular and they have the same
eigenvectors.
\label{lemma:ergodic-regular-rel}
#+end_lemma

#+begin_proof
$\frac{1}{2} (I + P)$ has all the diagonal elements postive and is stochastic: all the elements are positive and the
sum for the $i$ -th row is:

\[ \frac{1}{2} + \frac{1}{2} \sum_{j = 1}^m p_{ij} = \frac{1}{2} + \frac{1}{2} = 1 \]

To show that it is ergodic, fixed $i,j$, we must show that it exists a power matrix $\underbrace{\left\{\frac{1}{2}(I + P)\right\}}_{A}^m$ such that
$(A^m)_{ij} > 0$. Since $P$ is ergodic, it exists $P^k$ such that $(P^k)_{ij} > 0$, and
$\left\{\frac{1}{2}(I + P)\right\}^k = R + P^k$ where all of the elements of $R$ are non negative. Since its
diagonal is positive Lemma \eqref{lemma:ergodic-pos-diagonal} ends our proof.

Given $v: vP = v$, then $\frac{1}{2}(I + P)v = \frac{1}{2} (v + v) = v$.

To prove the other implication: let $v: \frac{1}{2}(I + P)v = v \implies \frac{1}{2}v + \frac{1}{2}P = v$,
and therefore $\frac{1}{2} P = \frac{1}{2}v$.
#+end_proof

#+begin_theorem
**Fundamental limit theorem for ergodic Markov chains**

If $P_{m\times m}$ is an ergodic transition matrix, then there exists a unique steady state $w$ with all its
elements strictly positive. Moreover, every left eigenvector $r: rP = r$ is linearly dependent of $w$, and
every right eigenvector $c: Pc = c$ is a multiple of $\onecol$.

\label{th:steady-ergodic}
#+end_theorem

#+begin_proof
Lemma \eqref{lemma:ergodic-regular-rel} and Theorem \eqref{th:steady-regular} suffice to reach the result.
#+end_proof

#+begin_lemma
An ergodic stochastic matrix $P_{m \times m}$ has a unique steady state $w$ that can be computed as a unique solution
of the system:

\[
\twopartsys{(I - P^t)w}{0}{\onerow \cdot w}{1}
\]
#+end_lemma

#+begin_proof
By means of Theorem \eqref{th:steady-ergodic} we have unicity and that the system

\[
  \left(\begin{array}{rcl}
    1 & 1 \ldots & 1 \\
    \hline
    & I - P^t & \\
  \end{array}\right) \cdot w =
  \left(\begin{array}{c}
    1 \\
    0 \\
    \vdots \\
    0
  \end{array}\right)
\]

must have a unique solution.
#+end_proof
** Steady states
We are now in conditions of giving of depicting how steady states of a general stochastic matrix can be computed.

#+begin_proposition
Given $P_{m\times m}$ an stochastic matrix, then every steady state $v$ assigns $0$ to the transient states, i.e. if
$i$ -th state is transient, then $(vP)_i = 0$

\label{prop:ss-transient-zero}
#+end_proposition

#+begin_proof
Looking at the canonic matrix from Equation \eqref{eq:canonic-form}, if $v$ is a steady state, then
if $[l, \ldots, m]$ are transient states (we can suppose w.l.o.g. they are ordered as in the canonic form), we would
have:

\[
(v_l, \ldots, v_m) W = (v_l, \ldots, v_m) \equiv (v_l, \ldots, v_m) \cdot (W - I) = 0
\]

$(W - I)$ is invertible by TODO. Hence, $(v_l, \ldots, v_m) = 0$

#+end_proof

Each matrix $P_1, \ldots, P_r$ from \eqref{eq:canonic-form} is irreducible. Therefore, by the fundamental Theorem \eqref{th:steady-ergodic},
each $P_i$ has a unique steady vector $v^{(i)}$. If $P_i$ corresponds to the states $[l_i, l_i + 1 \ldots, u_i]$ We are going
to call $\widetilde{v}^{(i)}_j = \twopartdef{0}{j < l_i \textrm{ or } j > u_i}{v^{(i)}_k}{j = l_{i + k - 1}}$.


#+begin_proposition
Given an stochastic matrix $P$ written in its canonic form, then its space of steady states is a convex hull given by:

\[
\bigg\{\sum_{i = 1}^r \alpha_i \widetilde{v}^{(i)}: 0 \le \alpha_i, \sum_{i = 1}^{r} \alpha_i = 1\bigg\}
\]

#+end_proposition

#+begin_proof
Since $P \sum_{i = 1}^r \alpha_i \widetilde{v}^{(i)} = \sum_{i = 1}^r \alpha_i P \widetilde{v}^{(i)} = \sum_{i = 1}^r \alpha_i \widetilde{v}^{(i)}$,
and $\sum_{j = 1}^m \bigg(\sum_{i = 1}^r \alpha_i \widetilde{v}^{(i)} \bigg)_j = \sum_{i = 1}^m \alpha_i \sum_{j = 1}^m \widetilde{v}^{(i)}_j = \sum_{i = 1}^m \alpha_i = 1$
then it is clear that every convex combination of $\widetilde{v}^{(i)}, i = 1, \ldots, r$ is a steady state for the matrix.

They are also linearly independent, trivially, since they are not null by the Theorem \eqref{th:steady-ergodic} and the positive regions
of each one do not overlap by construction.

Given a steady state $\widetilde{w}$, by the Proposition \eqref{prop:ss-transient-zero} we know $(\widetilde{w} P)_{j} = 0$ for $j= u_{r + 1}, \ldots, m$. And
by the shape of the matrix, $(P_i \widetilde{w})_j = v_j$ for each $j = l_j, \ldots, u_j$. By Theorem \eqref{th:steady-ergodic}, again, then
$w^{(i)} = (\widetilde{w}_{l_i}, \ldots, \widetilde{u_i})$ is $w^{(i)} = \alpha_i v^{(i)}$. But it has to hold:

\[
1 = \sum_{j = 1}^m \widetilde{w}_j = \sum_{i = 1}^{r} \sum_{j} w^{(i)}_j = \sum_{i = 1}^{r} \alpha_i \sum_{j} v^{(j)} = \sum_{i = 1}^{r} \alpha_i
\]
#+end_proof

** Mean first passage time
#+begin_definition
Given an ergodic Markov chain, we call the expected number of steps to reach a state $j$ from initial state $i$
as mean first passage time from $i$ to $j$. We note it $m_{ij}$. Formally:

\begin{eqnarray*}
  & \tau_{j} &= \min \{t: X_t = j, X_{l} \neq j, t >= 0, l < t\} \\
  & m_{ij}   &= \expecti{\tau_{j}}
\end{eqnarray*}

Obviously by the definition $m_{ii} = 0$
#+end_definition

#+begin_definition
If the initial state of an ergodic Markov is $i$, then we define the mean recurrence time for $i$ as:

\begin{eqnarray*}
  & \tau_{i}^{*} &= \min \{t: X_t = i, X_{l} \neq i, t > 0, l < t\} \\
  & r_{i}   &= \expecti{\tau_{i}^{*}}
\end{eqnarray*}
#+end_definition

#+begin_proposition
It holds

\begin{eqnarray*}
  & m_{ij}  &= 1 + \sum_{k \neq j} p_{ik} m_{kj} \\
  & m_{ij}   &= 1 + \sum_{k} p_{ik} m_{ki}
\end{eqnarray*}

\label{prop:mfpt-rec}
#+end_proposition

#+begin_proof
The following recurrence holds:

\[
  m_{ij} = p_{ij} + \sum_{k \neq j} p_{ik} (m_{kj} + 1)
  \underset{\sum_{k} p_{ik} = 1}{=} 1 + \sum_{k \neq j} p_{ik} m_{kj}
\]

Analogously:

\[
  r_i = \sum_{k} p_{ik}(m_{ki} + 1) = 1 + \sum_{k} p_{ik} m_{ki}
\]
#+end_proof

Matricially, we can define $M = (m_{ij})$, $D = (\delta_{ij} \cdot r_i)$,
$C = \left(\begin{array}{cccc} 1 & 1 &\ldots & 1 \\
                               1 & 1 &\ldots & 1 \\
                              \vdots & \vdots & \ddots & \vdots \\
                               1 & 1 &\ldots & 1 \end{array}\right)$

And for the recurrence \eqref{prop:mfpt-rec} we have:

\begin{equation}
M = PM + C - D
\label{prop:mfpt-rec-mat}
\end{equation}

#+begin_proposition
Given an ergodic Markov chain, then $r_i = w_i^{-1}$ where $w$ is its unique steady state.

\label{prop:ss-and-recurrence-prob}
#+end_proposition

#+begin_proof
By Equation \eqref{prop:mfpt-rec-mat} we have $w(I - P)M = 0 = w(C -D)$.

Therefore $w\cdot C = \onerow = (w_1\cdot r_1, \ldots w_n r_n)$, and $r_i = w_i^{-1}$
#+end_proof

#+begin_proposition
Let $P_{m\times m}$ an ergodic matrix, $w$ its steady state and $W$ the matrix formed by $w$ as rows.
Then $I - P + W$ is invertible and $Z = (I - P + W)^{-1}$ holds:

1. $Z \cdot \onecol = \onecol$, $w\cdot Z = w$
2. $Z(I - P) = I - W$
\label{prop:z-invertible}
#+end_proposition

#+begin_proof
Let $x$ such that $(I - P + W)x = 0$. Since $w P = w$ then $\{w P^n \} \rightarrow wW = w$ and similarly
$W \onecol = \onecol$. Therefore $w(I - P + W)x = wx = 0 \implies Wx = 0 \implies (I - P)x = 0$. Hence,
this means $Px = x$. By Theorem \eqref{th:steady-ergodic} this implies
$x = \lambda \onecol$, but $wx = 0$ and $w$ has only positive elements. Therefore $x = 0$, and we can use
$A_{m \times m}$ is invertible iff $\forall x \in \reals^m, Ax = 0 \implies x = 0$.

$\onecol = (I - P + W)c$ and multiplying by $Z$ in both sides: $Zc = C$. Similarly, we can show $wZ = w$.

Since $Zc$ and $ZW = Z \left(\begin{array}{c|c|c} w_1\onecol & \ldots & w_m\onecol \end{array}\right) =
W$. Then $Z(I - P) = Z(I -P + W - W) = I - ZW = I - W$.
#+end_proof

#+begin_theorem
Given $P$ an ergodic matrix, $w$ its steady state, $Z = (I - P + W)^{-1} = (z_{ij})$ and $M = (m_{ij})$,
it holds:

\[ m_{ij} = \frac{z_{jj} - z_{ij}}{w_j} \]

\label{th:mfpt-computation}
#+end_theorem

#+begin_proof
From \eqref{prop:mfpt-rec-mat} we have $(I - P) M = C - D$ and:

\[
Z(I - P)M = ZC - ZD \underset{\textrm{Lemma \eqref{prop:z-invertible}}}{\dimplies} M - WM = C -ZD
\dimplies M = C -ZD + WM
\]

and from that equation an $m_{jj} = 0$, we deduce:

\[
  (w M)_{j} = \twopartdef{z_{ij}r_j - 1 - m_{ij}}{i \neq j}{z_{jj}r_j - 1}{i = j}
\]

and therefore, subtracting and using Proposition \eqref{prop:ss-and-recurrence-prob}:

\[ m_{ij} = \frac{z_{jj} - z_{ij}}{w_j} \]
#+end_proof

* =markovchain= package
We can propose an alternative definition of communicating property, taking $n \ge 0$ instead of $n > 0$,
which has been the one used in the software from the start:

#+begin_definition
The state $i\in E$ communicates with $j\in S$, and we will write it like $i \rightarrow j$
iff $p_{i,j}^{(n)} > 0$ for some $n \ge 0$.
#+end_definition

Therefore, we can reformualte Proposition \eqref{prop:equiv-classes} as:

#+begin_proposition
$\dgoto$ is an equivalence relationship in $S^{\ast}$, and we can decompose:

\[
S = C_1 \cup C_2 \cup \ldots \cup C_r
\]

where for every $i, j \in C_k$, it holds $i \dgoto j$, and if $i \in C_k, j \in C_l$ where $k \neq l$, then $i \not\goto j$ or $j \not\goto i$.
#+end_proposition

The Corollary \eqref{corollary:comm-classes} is still valid with this definition, and gets used in the software.

** Communicating classes based algorithms
The original algorithm to compute the communicating classes of a given markovchain, and whether
they were closed or not, was based on \cite{feres}. The algorithm runs in $\algcomp{n^3}$ where
$n$ is the number of states of the input Markov chain. The problem is that it is excessively
inefficient when it comes to easy inputs.

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \NEWLINE
  \FOR{$i \in \{1, \ldots, m\}$}
    \STATE{$b = (\underset{(1)}{false}, \ldots \underset{(m)}{false})$}
    \STATE{$b[i] \leftarrow true$ since $p_{i,i}^{(0)} = 1$}
    \STATE{$k' = 0$}
    \STATE{$k = 1$}
    \STATE{$a = [i]$ a list with the states $i$ can communicate with}
    \NEWLINE
    \WHILE{$k' \neq k$}
      \STATE{$k' = \sum_{l = 1}^{n} \mathbb{1}_{b[l] = true}$}
      \STATE{Set $c[l] = \sum_{k \in a} (P(k,i))$ for $l \in \{1, \ldots n\}$}
      \STATE{Set $a = \{l: c[l] > 0, l \in \{1, \ldots n\}\}$}
      \STATE{Set $b[l] = true$ for each $l \in a$ and $k = \card{a}$}
    \ENDWHILE
    \NEWLINE
    \STATE{Set $T(i,\_) = b$}
  \ENDFOR
  \NEWLINE
  \STATE{Set $C(i,j) = T(j, i) \textrm{ and } T(i, j)$ (it holds $i \goto j$ and $j \goto i$)}
  \STATE{Set $v[i] = \textrm{ true iff } \card{ \{j: C(i,j) = true \} }= \card{ \{j: T(i,j) = true \} }$}
  \NEWLINE
  \RETURN{$[C, v]$, where $C(i,j)$ is true iff $i,j$ are in the same communicating class,
          and $v[i]$ is true iff the class for $i$ is closed}
\end{algorithmic}
\caption{Original \texttt{commClassesKernel} algorithm}
\label{alg:commClassesKernelOld}
\end{algorithm}


\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \STATE{Parse the graph for the states keeping $S[i]$ an adjacency list for
          each state $i \in \{1, \ldots, m\}$}
  \NEWLINE
  \FOR{$i \in \{1, \ldots, m\}$}
    \STATE{$a = stack \{ i \}$ a list with the states $i$ can communicate with}
    \STATE{$b = (\underset{(1)}{false}, \ldots \underset{(m)}{false})$}
    \WHILE{$a$ is not empty}
       \STATE{$k =$ \texttt{a.pop()}}
       \STATE{$T(i,k) = true$}
       \FOR{$l \in S[k]$}
         \IF{$T(i,l) == false$}
           \STATE{\texttt{a.push(l)}}
         \ENDIF
       \ENDFOR
    \ENDWHILE
  \ENDFOR
  \NEWLINE
  \STATE{Set $C(i,j) = T(j, i) \textrm{ and } T(i, j)$ (it holds $i \goto j$ and $j \goto i$)}
  \STATE{Set $v[i] = \textrm{ true iff } \card{ \{j: C(i,j) = true \} }= \card{ \{j: T(i,j) = true \} }$}
  \NEWLINE
  \RETURN{$[C, v]$, where $C(i,j)$ is true iff $i,j$ are in the same communicating class,
          and $v[i]$ is true iff the class for $i$ is closed}
\end{algorithmic}
\caption{\texttt{commClassesKernel} algorithm}
\label{alg:commClassesKernel}
\end{algorithm}


For the homogeneous Markov chain with transition matrix as follows
\[
\left(\begin{array}{ccccc}
0 & 1 & 0 & \ldots & 0\\
0 & 0 & 1 & \ldots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & 1 \\
1 & 0 & 0 & \ldots & 0
\end{array}\right)
\]

the original algorithm would run in $\algcomp{n^4}$, whereas our improved algorithm would
run in $\algcomp{n^3}$, due to the adjacency lists and the \textit{backtracking} approach to
visit all the nodes In the general case (a matrix with $\approx n^2$ non-zero entries,
complexity of our algorithm would be $\algcomp{n^3}$ and the old one would be $\algcomp{n^4}$
still.

*** Communicating classes

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \STATE{Do $[C, v] = \textrm{\texttt{commClassesKernel}}$}
  \STATE{Mark all states' class as not computed :
         $w = (\underset{(1)}{false}, \ldots, \underset{(m)}{false})$
  }
  \STATE{Initialize $A = \emptyset$}
  \NEWLINE
  \FOR{$i \in \{1, \ldots, m\}$}
    \STATE{$B = \emptyset$}
    \IF{not $w_i$ (class not computed)}
      \FOR{$j \in \{1, \ldots, m\}$}
         \IF{$C(i,j)$}
            \STATE{$B := B \bigcup \{j\}$}
            \STATE{$w_j := true$}
         \ENDIF
      \ENDFOR
      \STATE{$A := A \bigcup \{B\}$}
    \ENDIF
  \ENDFOR
  \NEWLINE
  \RETURN{$A$}
\end{algorithmic}
\caption{\texttt{communicatingClasses} algorithm}
\label{alg:commClasses}
\end{algorithm}

*** Transient and recurrent classes / states
For Markov chains with a finite number of states, Corollary \eqref{corollary:comm-recurrence}
guarantees us that recurrent classes and closed classes are the same [fn::[[https://github.com/spedygiorgio/markovchain/issues/171][markovchain issue#171]]]

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \IF{$P$ is stochastic by columns}
    \STATE{$P := P^t$}
  \ENDIF
  \STATE{Do $[C, v] = \textrm{\texttt{commClassesKernel}}(P)$}
  \STATE{Mark all states' class as not computed :
         $w = (\underset{(1)}{false}, \ldots, \underset{(m)}{false})$
  }
  \STATE{Initialize $A = \emptyset$}
  \NEWLINE
  \FOR{$i \in \{1, \ldots, m\}$}
    \STATE{$B = \emptyset$}
    \IF{$v_i$ (class closed) and not $w_i$ (class not computed)}
      \FOR{$j \in \{1, \ldots, m\}$}
         \IF{$C(i,j)$}
            \STATE{$B := B \bigcup \{j\}$}
            \STATE{$w_j := true$}
         \ENDIF
      \ENDFOR
      \STATE{$A := A \bigcup \{B\}$}
    \ENDIF
  \ENDFOR
  \NEWLINE
  \RETURN{$A$}
\end{algorithmic}
\caption{\texttt{recurrentClasses} algorithm}
\label{alg:recurrentClasses}
\end{algorithm}

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \IF{$P$ is stochastic by columns}
    \STATE{$P := P^t$}
  \ENDIF
  \STATE{Do $[C, v] = \textrm{\texttt{commClassesKernel}}(P)$}
  \STATE{Mark all states' class as not computed :
         $w = (\underset{(1)}{false}, \ldots, \underset{(m)}{false})$
  }
  \STATE{Initialize $A = \emptyset$}
  \NEWLINE
  \FOR{$i \in \{1, \ldots, m\}$}
    \STATE{$B = \emptyset$}
    \IF{not $v_i$ (class not closed) and not $w_i$ (class not computed)}
      \FOR{$j \in \{1, \ldots, m\}$}
         \IF{$C(i,j)$}
            \STATE{$B := B \bigcup \{j\}$}
            \STATE{$w_j := true$}
         \ENDIF
      \ENDFOR
      \STATE{$A := A \bigcup \{B\}$}
    \ENDIF
  \ENDFOR
  \NEWLINE
  \RETURN{$A$}
\end{algorithmic}
\caption{\texttt{transientClasses} algorithm}
\label{alg:transientClasses}
\end{algorithm}

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \IF{$P$ is stochastic by columns}
    \STATE{$P := P^t$}
  \ENDIF
  \STATE{Do $[C, v] = \textrm{\texttt{commClassesKernel}}(P)$}
  \STATE{Initialize $A = \emptyset$}
  \NEWLINE
  \FOR{$i \in \{1, \ldots, m\}$}
    \IF{$w_i$}
      \STATE{$A := A \bigcup \{i\}$}
    \ENDIF
  \ENDFOR
  \NEWLINE
  \RETURN{$A$}
\end{algorithmic}
\caption{\texttt{recurrentStates} algorithm}
\label{alg:recurrentStates}
\end{algorithm}

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \IF{$P$ is stochastic by columns}
    \STATE{$P := P^t$}
  \ENDIF
  \STATE{Do $[C, v] = \textrm{\texttt{commClassesKernel}}(P)$}
  \STATE{Initialize $A = \emptyset$}
  \NEWLINE
  \FOR{$i \in \{1, \ldots, m\}$}
    \IF{not $w_i$}
      \STATE{$A := A \bigcup \{i\}$}
    \ENDIF
  \ENDFOR
  \NEWLINE
  \RETURN{$A$}
\end{algorithmic}
\caption{\texttt{transientStates} algorithm}
\label{alg:transientStates}
\end{algorithm}

*** Steady States

**** First approach

First idea was to try computing the eigen vectors for the eigen value $1$. If some negative value arose (even after normalizing and obtaining probability vectors), then we could refine the approach of using the recurrent states to compute the steady states.

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \IF{$P$ is stochastic by rows}
    \STATE{$P := P^t$}
  \ENDIF
  \STATE{Call $S = \textrm{\texttt{computeSteadyStates}}(P)$}
  \IF{There is any negative element in $S$}
    \STATE{Compute $S =$\texttt{steadyStatesByRecurrentClasses}$(P)$}
  \ENDIF
  \NEWLINE
  \IF{$P$ was stochastic by rows originally}
     \STATE{$S:= S^t$}
  \ENDIF
  \NEWLINE
  \RETURN{$S$}
\end{algorithmic}
\caption{\texttt{steadyStates} algorithm}
\label{alg:steadyStatesFirst}
\end{algorithm}


\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix by columns
  \STATE{Compute $v = (e_1, \ldots, e_k)$, eigen values for $P$, i.e.
    \[
      \forall e_j \in \mathbb{C} \quad \exists v_j =
              \left(\begin{array}{c}
                v_j^{(1)} \\
                \vdots \\
                v_j^{(m)}
              \end{array}\right)
      \in \mathbb{C}^m: Pv_i = e_k v_i
    \]
  }
  \STATE{Compute $S = (Re(v_1), \ldots, Re(v_l)) = (u_1, \ldots, u_l)$, eigen vectors for eigen values $1$}
  \FOR{$j = 1, \ldots, l$}
    \STATE{$s_j = \sum_{i = 1}^m u_j^{(i)}$}
    \IF{$s_j$ is $0$}
      \STATE{$s_j = 1$}
    \ENDIF
  \ENDFOR
  \STATE{Normalize each column $u_j := u_j / s_j$}
  \NEWLINE
  \RETURN{$S = (u_1, \ldots, u_l)$}
\end{algorithmic}
\caption{\texttt{computeSteadyStates} algorithm}
\label{alg:computeSteadyStatesFirst}
\end{algorithm}


Note the step of taking $(u_1, \ldots, u_l) = (Re(v_1), \ldots, Re(v_l))$ is fundamented in the fact that every matrix if $e_j \in \mathbb{R}$ is eigen value for $P$, and $P \cdot u_j = e_j \cdot v_j$, i.e. $u_j$ is an eigen vector tied to $e_j$, then $Re(v_j$ and $Im(v_j)$ are also eigen vectors tied to $e_j$.

However, the aforementioned algorithm, as the original one, has several problems:

1. The computation of eigen values for a matrix in the general case, as described in [fn::[[https://en.wikipedia.org/wiki/Eigenvalue_algorithm#Iterative_algorithms][eigenAlgorithm]]] can take up to $\algcomp{m^3}$ or $\algcomp{m^4}$ and since iterative algorithms and their convergence is used, we could end up having rounding problems (i.e. how to securely search for the $1$ eigen values).
2. Taking real parts does not ensure that we end up with linearly independent eigen vectors. The original algorithm also presented this problem. Let us propose a counterexample of this. If the matrix $P$ has real eigen vectors $\{u, v\}$ which are linearly independent and are tied to the eigen value $1$, $\{u + iu, u + iv\}$ are also independent eigen vectors whore real parts are $\{u,u\}$. We cannot assure that the eigen values computation method is going to output real eigen vectors only (we can only make assurances in the case the matrix $P$ is symmetric).

As result, we could be using this algorithm and obtain less steady states than we should.


\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix by columns
  \STATE{$C = \textrm{\texttt{recurrentClasses}}(P) = \{C_1, \ldots, C_l\}$, where for all $k=1, \ldots, l$
         $C_k = \{i^{(k)}_1, \ldots, i^{(k)}_{l_k}\} \subseteq \{1, \ldots, m\}$ is a recurrent class}
  \NEWLINE
  \FOR{$k = 1, \ldots, l$}
    \STATE{Compute the submatrix $P_k$ from the matrix \eqref{eq:canonic-form},
           subseting $P_k = P[C_k, C_k]$}
    \STATE{Call $v_k = \textrm{\texttt{computeSteadyStates}}(P_k)$}
    \IF{Some element from $v_k$ is negative}
      \STATE{Stop the algorithm, we cannot compute steady states}
    \ENDIF
    \STATE{Else do
      \[
        u_j = \twopartdefow{v_t}{j = i^{(k)}_t \textrm{ for some } t \in \{1, \ldots l_k\}}{0}
      \]
    }
  \ENDFOR
  \NEWLINE
  \RETURN{$S = (u_1, \ldots u_l)$}
\end{algorithmic}
\caption{\texttt{steadyStatesByRecurrentClasses} algorithm}
\label{alg:steadyStatesByRecurrentClassesFirst}
\end{algorithm}

**** Second approach

We could think of using only \eqref{alg:steadyStatesByRecurrentClassesFirst} and drop \eqref{alg:computeSteadyStatesFirst},
and therefore use only the recurrent classes to compute the steady states. But problem of not having assurances about the
convergence (and later search for the unitary eigen values) and the algorithmic complexity of the method pushed us to find
 another algebraic approach with guaranteed algorithmic complexity of $\algcomp{m^3}$.

Since we want to find for $k=1, \ldots, l$ $P_k \cdot u_k = u_k$, and by the theorem TODO each block has a unique steady state, we know the
kernel of $P_k - I$ has dimension $1$ (because $u_k$ is not null and if it had more dimensionality than $1$ we would have more steady states).
Hence, we cannot solve $(\alpha_{ij}) u_k = (P_k - I) \cdot u_k = 0$ with an unique solution $u_k$, but we can solve:

\[
\widetilde{P}_k = (\widetilde{p}_ij) = \twopartdefow{1}{i = 1}{\alpha_{ik}} \qquad
\widetilde{P}_k \cdot u_k = \left(\begin{array}{c} 1 \\ 0 \\ \vdots 0 \end{array}\right)
\]

Solving this last system for each $k = 1, \ldots, l$ can be done in $\algcomp{m^2}$. Therefore, the total cost of the algorithm, since computing recurrent classes is $\algcomp{m^3}$, is $\algcomp{m^3}$, as we were aiming for.


\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix by columns
  \IF{$P$ is stochastic by rows}
    \STATE{$P := P^t$}
  \ENDIF
  \STATE{$C = \textrm{\texttt{recurrentClasses}}(P) = \{C_1, \ldots, C_l\}$, where for all $k=1, \ldots, l$
         $C_k = \{i^{(k)}_1, \ldots, i^{(k)}_{l_k}\} \subseteq \{1, \ldots, m\}$ is a recurrent class}
  \NEWLINE
  \FOR{$k = 1, \ldots, l$}
    \STATE{Compute the submatrix $P_k$ from the matrix \eqref{eq:canonic-form},
           subseting $P_k = P[C_k, C_k]$}
    \STATE{Compute $\widetilde{P}_k$}
    \STATE{Compute $u_k$ solving the system:
      \[
         \widetilde{P}_k \cdot u_k = \left(\begin{array}{c} 1 \\ 0 \\ \vdots 0 \end{array}\right)
      \]
    }
  \ENDFOR
  \NEWLINE
  \STATE{$S = (u_1, \ldots u_l)$}
  \IF{$P$ was stochastic by rows originally}
     \STATE{$S:= S^t$}
  \ENDIF
  \NEWLINE
  \RETURN{$S$}
\end{algorithmic}
\caption{\texttt{steadyStates} algorithm}
\label{alg:steadyStates}
\end{algorithm}

** Markov chain classification

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \NEWLINE
  \STATE{${C_1, \ldots, C_n} = \texttt{CommunicatingClasses}(P)$}
  \IF{$n = 1$}
    \RETURN{True}
  \ELSE
    \RETURN{False}
  \ENDIF
\end{algorithmic}
\caption{\texttt{is.irreducible} algorithm}
\label{alg:isIrreducible}
\end{algorithm}

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \REQUIRE $n$ the exponent
  \NEWLINE
  \STATE{$R:= I_{m \times m}$}
  \STATE{$A = P$}
  \WHILE{$n > 0$}
    \IF{$n mod(2) = 1$}
      \STATE{$R := R + A$}
    \ENDIF
    \STATE{$A = A * A$}
    \STATE{$n := n / 2$}
  \ENDWHILE
  \RETURN{$R$}
\end{algorithmic}
\caption{$\algcomp{log(n)}$ \texttt{matrixPow} algorithm}
\label{alg:matrixPow}
\end{algorithm}

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \NEWLINE
  \IF{Some element $p_ii > 0$}
    \STATE{$d = \#\{i \in \{1, \ldots, m\}: p_{ii} > 0\}$}
    \RETURN{$\texttt{matrixPow}(P, 2m - d - 1) > 0$}
  \ELSE
    \STATE{$\texttt{matrixPow}(P, m^2 - 2m + 2) > 0$}
  \ENDIF
\end{algorithmic}
\caption{\texttt{is.regular} algorithm}
\label{alg:isRegular}
\end{algorithm}

** Absorption
\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \NEWLINE
  \STATE{$T = \texttt{transientStates}(P)$}
  \STATE{$R = \texttt{recurrentStates}(R)$}
  \STATE{Compute $W = P[T, T]$}
  \STATE{Compute the fundamental matrix $N = (I - W)^{-1}$}
  \STATE{Compute $t = N \cdot c$}
  \RETURN{$t$}
\caption{\texttt{meanAbsorptionTime} algorithm}
\label{alg:meanAbsorptionTime}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \NEWLINE
  \STATE{$T = \texttt{transientStates}(P)$}
  \STATE{$R = \texttt{recurrentStates}(R)$}
  \STATE{Compute $W = P[T, T]$}
  \STATE{Compute the fundamental matrix $N = (I - W)^{-1}$}
  \STATE{Compute $F^{*} = N \cdot P[transient, recurrent$}
  \RETURN{$F^{\ast}$}
\caption{\texttt{absorptionProbabilities} algorithm}
\label{alg:absorptionProbabilities}
\end{algorithmic}
\end{algorithm}
** Hitting probabilities
We want to come up with an algorithm to compute the hitting probabilities for a Markov chain.  We have not found anything documented in the literature with respect to the computation of these probabilities algorithmically, so we had tried to come up with our own algorithm.

The first thought would be to re-write the system \eqref{eq:fij-recurrence} matrix-wise. Let us
remember those equations:

\[
f_{i,j} = p_{i,j} + \sum_{k \neq j} p_{i,k} f_{k,j}
\]

Rewriting the system:

\begin{equation*}
A = \left(\begin{array}{c|c|c|c}
  A_1 & \bigzero & \ldots & \bigzero \\
\hline
  \bigzero & A_2 & \ldots & \bigzero \\
\hline
  \vdots & \vdots & \ddots & \bigzero \\
\hline
  \bigzero & \bigzero & \ldots & A_n
\end{array}\right)
\end{equation*}

\begin{eqnarray*}
A_1 &=
\left(\begin{matrix}
  -1     & p_{1,2}       & p_{1,3}   & \ldots & p_{1,n} \\
  0      & (p_{2,2} - 1) & p_{2,3}   & \ldots & p_{2,n} \\
  \vdots & \vdots        & \vdots    & \ddots & \vdots  \\
  0      & p_{n, 2}      & p_{n,3}   & \ldots & (p_{n,n} - 1)
  \end{matrix}\right)\\
A_2 &= \left(\begin{matrix}
  (p_{1,1} - 1) & 0      & p_{1,3}   & \ldots & p_{1,n} \\
  p_{2,1}       & -1     & p_{2,3}   & \ldots & p_{2,n} \\
  \vdots        & \vdots & \vdots    & \ddots & \vdots  \\
  p_{n,1}       & 0      & p_{n,3}   & \ldots & (p_{n,n} - 1)
  \end{matrix}\right)\\
\vdots & \vdots\\
A_n &= \left(\begin{matrix}
  (p_{1,1} - 1) & p_{1,2}      & p_{1,3}   & \ldots & 0 \\
  p_{2,1}       & (p_{2,2} -1) & p_{2,3}   & \ldots & 0 \\
  \vdots        & \vdots       & \vdots    & \ddots & \vdots  \\
  p_{n,1}       & p_{n,2}      & p_{n,3}   & \ldots & -1
  \end{matrix}\right)\\
\end{eqnarray*}

\begin{equation*}
\begin{array}{rlrl}
X = \left(\begin{array}{c}
f_{1,1} \\
f_{2,1} \\
\vdots  \\
f_{n,1} \\
f_{1,2} \\
\vdots  \\
f_{n,2} \\
\vdots  \\
f_{1,n} \\
\vdots  \\
f_{n,n}
\end{array}\right) &
X_j = \left(\begin{array}{c}
f_{1,j} \\
f_{2,j} \\
\vdots  \\
f_{n,j}
\end{array}\right) &
C = - \left(\begin{array}{c}
p_{1,1} \\
p_{2,1} \\
\vdots  \\
p_{n,1} \\
p_{2,2} \\
\vdots  \\
p_{n,2} \\
\vdots  \\
p_{1,n} \\
\vdots  \\
p_{n,n}
\end{array}\right)
\end{array}
\end{equation*}

And we end up with the system:

\begin{equation}
AX = C
\end{equation}

Let us remember that standard algorithms to compute the inverse of a matrix are $\algcomp{n^3}$ where $n$ is the number of rows / columns of the matrix. Clearly, we can solve this system, provided that it has solution, blockwise, solving $A_j X_j = C_j$, but the matrix $A$ may not be invertible in some cases. For example let us imagine the $i$ -th state has transition probabilities: $(0, \ldots, 0, \underset{i)}{1}, 0, \ldots, 0)$. Then that same row would turn into $(0,0, \ldots, 0)$ for some block, thus obtaining a singular matrix. Another case which may give us problems could be: state $i$ has the following transition probabilities: $(0, \ldots, 0, \underset{j)}{1}, 0, \ldots, 0)$ and the state $j$ has the following transition probabilities: $(0, \ldots, 0, \underset{i)}{1}, 0, \ldots, 0)$. Then when constructing some blocks we will end up with rows:

\begin{eqnarray*}
(0, \ldots, 0, \underset{i)}{-1}, 0, \ldots, 0, \underset{j)}{1}, 0, \ldots, 0) \\
(0, \ldots, 0, \underset{i)}{1},  0, \ldots, 0, \underset{j)}{-1}, 0, \ldots, 0)
\end{eqnarray*}

which are linearly dependent. Our hypothesis is that if we treat the closed communicating classes differently (see \eqref{eq:canonic-form} for context), we are deleting the linearity in the system. This is an untested hypothesis of course, but computationally, the difference between proving it or not would be minimal (let us suppose we prove it, then some matrixes would still not be invertible due to numeric errors, numbers very close to zero, or to the overflow, etc, so the corresponding method would still have to perform a warning for not solvable problems).

In conclusion using \eqref{corollary:comm-recurrence} and \eqref{prop:recurrent-classes}, if we have a closed communicating class $C$, then $f_{i,j} = 1$ for all $i,j \in C$ and $f_{k,j}  = 0$ for all $k\not\in C$. The algorithm we get would be something alike:

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \STATE{\texttt{communicating} = Communicating classes for $P$}
  \STATE{\texttt{closed} = Boolean vector where the $i$ -th position is true iff the communicating class for $i$ is closed}
  \NEWLINE
  \FOR{$j \in \{1, \ldots, m\}$}
    \STATE{Compute the matrix $A_j$}
    \STATE{Compute the indepent terms $C_j$}
    \FOR{$i \in \{1, \ldots, m\}$}
      \IF{\texttt{closed}(i)}
        \FOR{$k \in \{1, \ldots, m\}$}
           \IF{$k \neq i$}
              \STATE{$A_j(i, k) = 0$}
           \ELSE
              \STATE{$A_j(i, i) = 1$}
           \ENDIF
        \ENDFOR
        \NEWLINE
        \IF{\texttt{communicating}(i, j)}
          \STATE{$C_j(i) = 1$}
        \ELSE
          \STATE{$C_j(i) = 0$}
        \ENDIF
      \ENDIF
    \ENDFOR
    \NEWLINE
    \STATE{Compute $X_j = A_j^{-1} \cdot C_j$}
  \ENDFOR
  \RETURN{$X = \left(\begin{array}{c}
               X_1 \\
               \vdots \\
               X_m
               \end{array}\right)$
         }
\end{algorithmic}
\caption{Algorithm to compute hitting probabilities}
\label{alg:hittingProbs}
\end{algorithm}

** Mean first passage time and recurrence times
Based on Theorem \eqref{th:mfpt-computation}, we ported and polished the following algorithm to C++ (a
version of it was already in the package, although it did not made checkings about the matrix being
regular). Due to backward compatibility with the previous version, the algorithm can be called with an
ergodic matrix and a set of states \texttt{destination} and it would compute the mean first passage time
from the states not in \texttt{destination} to any of the states there.

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an \textbf{ergodic} matrix
  \REQUIRE \texttt{destination} a subset of $\{1, \ldots, m \}$. Empty by default.
  \NEWLINE
  \IF{\texttt{destination} is empty}
    \STATE{Compute $w = \texttt{steadyStates}(P)$ (there should be only one)}
    \STATE{Compute $Z = (I - P + W)^{-1}$}
    \STATE{Compute $m_{ij} = (Z_{jj} - Z_{ij}) * w^{-j}$ for all $i, j \in \{1, \ldots, m\}$}
    \RETURN{$M = (m_{ij})$}
  \ELSE
    \STATE{$M = \texttt{computeMeanAborptionTimes}(P, \texttt{destination})$}
  \ENDIF
\end{algorithmic}
\caption{\texttt{meanFirstPassageTime} algorithm}
\label{alg:mfpt}
\end{algorithm}

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \REQUIRE $P_{m \times m}$ an stochastic matrix
  \STATE{$[u_1, \ldots, u_n] = \texttt{steadyStates}(P)$}
  \STATE{$r = (0, \ldots, 0)$}
  \FOR{Each steady state $u_i$}
    \FOR{$j = 1, \ldots, m$}
      \IF{$u_{ij} > 0$}
         \STATE{$r_j := u_{ij}^{-1}$}
      \ENDIF
    \ENDFOR
  \ENDFOR
  \RETURN{$r = \{(j, r_j): r_j > 0\}$}
\end{algorithmic}
\caption{\texttt{meanRecurrenceTime} algorithm}
\label{alg:mrt}
\end{algorithm}

* Conclusions and future work
 #+LaTeX: \appendix
* Appendix
** Installation of markovchain
As prerequisite, R [fn::[[https://cloud.r-project.org][https://cloud.r-project.org]]] should be installed. In \textit{Debian} based distributions can be installed as:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src bash :eval no
  sudo apt install r-base rbase-dev
#+end_src

In \textit{Arch Linux} based distributions as:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src bash :eval no
  sudo pacman -S r
#+end_src

It could be useful to have RStudio [fn::[[https://www.rstudio.com][https://www.rstudio.com]]] installed too (although it is not mandatory to execute the software).

To install the =markovchain= [fn::[[https://github.com/spedygiorgio/markovchain][https://github.com/spedygiorgio/markovchain]]] package, one could do (from an R terminal or inside RStudio):

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :eval no
  install.packages("markovchain")
#+end_src

** Examples
First we need to load =markovchain=

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output silent
library("markovchain")
#+end_src

We define the data. We will be using Drunkard's Walk from \cite{grinstead}.

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
# Define Drunkard's Walk transition matrix
# It is a by row stochastic marix
drunkardsMatrix <- matrix(c(  1,   0,   0,   0,   0,
                            0.5,   0, 0.5,   0,   0,
                              0, 0.5,   0, 0.5,   0,
                              0,   0, 0.5,   0, 0.5,
                              0,   0,   0,   0,   1), nrow = 5, byrow = TRUE)
# Define states as 1: home, 5: bar, rest numbered corners
states <- c("home", "2", "3", "4", "bar")
# Create the Markov chain from the matrix
mc <- new("markovchain", name = "Drunkard's walk",
          transitionMatrix = drunkardsMatrix, states = states)
mc
#+end_src

#+RESULTS:
#+begin_example
Drunkard's walk
 A  5 - dimensional discrete Markov Chain defined by the following states:
 home, 2, 3, 4, bar
 The transition matrix  (by rows)  is defined as follows:
     home   2   3   4 bar
home  1.0 0.0 0.0 0.0 0.0
2     0.5 0.0 0.5 0.0 0.0
3     0.0 0.5 0.0 0.5 0.0
4     0.0 0.0 0.5 0.0 0.5
bar   0.0 0.0 0.0 0.0 1.0
#+end_example

Communicating classes are $\{\textrm{home}\}, \{2, 3, 4\}, \{\textrm{bar}\}$:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
communicatingClasses(mc)
#+end_src

#+RESULTS:
: [[1]]
: [1] "home"
:
: [[2]]
: [1] "2" "3" "4"
:
: [[3]]
: [1] "bar"


#+begin_src R :session examples :results output silent :exports none
set.seed(450)
#+end_src

#+attr_latex: :options style=vs, bgcolor=light-gray
#+header: :width 5 :height 5 :R-dev-args
#+begin_src R :session examples :results output graphics :file imgs/drunkards-mc.svg :exports both :cache yes
plot(mc, package = "diagram", box.size = 0.1)
#+end_src

#+RESULTS:
[[file:imgs/drunkards-mc.svg]]


Recurrent states are:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
recurrentStates(mc)
#+end_src

#+RESULTS:
: [1] "home" "bar"

which of course are absorbing:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
recurrentStates(mc) == absorbingStates(mc)
#+end_src

#+RESULTS:
: [1] TRUE TRUE

Transient states are:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
transientStates(mc)
#+end_src

#+RESULTS:
: [1] "2" "3" "4"

Hitting probabilities are:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
hittingProbabilities(mc)
#+end_src

#+RESULTS:
:      home         2   3         4  bar
: home 1.00 0.0000000 0.0 0.0000000 0.00
: 2    0.75 0.3333333 0.5 0.3333333 0.25
: 3    0.50 0.6666667 0.5 0.6666667 0.50
: 4    0.25 0.3333333 0.5 0.3333333 0.75
: bar  0.00 0.0000000 0.0 0.0000000 1.00

which means that if the drunk person starts in corner $2$ (state $2$) the probability of ever returning
there is $0.333333$, of ever reaching home, $0.75$ and of ever reaching the bar, $0.25$. Of course this
last two former probabilities should coincide with the =absorptionProbabilities=, since they are absorbing
states:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
absorptionProbabilities(mc)
#+end_src

#+RESULTS:
:   home  bar
: 2 0.75 0.25
: 3 0.50 0.50
: 4 0.25 0.75

And the mean absorption time (that is, how much is the drunk person going to take to go home / bar in average) is given by:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
hittingProbabilities(mc)
#+end_src

#+RESULTS:
:      home         2   3         4  bar
: home 1.00 0.0000000 0.0 0.0000000 0.00
: 2    0.75 0.3333333 0.5 0.3333333 0.25
: 3    0.50 0.6666667 0.5 0.6666667 0.50
: 4    0.25 0.3333333 0.5 0.3333333 0.75
: bar  0.00 0.0000000 0.0 0.0000000 1.00

which means that if the drunk person starts in corner $2$ (state $2$) the probability of ever returning
there is $0.333333$, of ever reaching home, $0.75$ and of ever reaching the bar, $0.25$. Of course this
last two former probabilities should coincide with the =absorptionProbabilities=, since they are absorbing
states:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
absorptionProbabilities(mc)
#+end_src

#+RESULTS:
:   home  bar
: 2 0.75 0.25
: 3 0.50 0.50
: 4 0.25 0.75

And the mean absorption time (that is, how many setps going to take for the drunk person to reach home /
the bar in average) from each corner is:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
meanAbsorptionTime(mc)
#+end_src

#+RESULTS:
: 2 3 4
: 3 4 3

that is, $3$ steps if that person start in corner $2, 4$, otherwise $4$ steps.

To compute the steady states of the matrix:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
steadyStates(mc)
#+end_src

The number of visits to each state $j$ if we start in $i$:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
meanNumVisits(mc)
#+end_src

#+RESULTS:
:      home   2 3   4 bar
: home  Inf 0.0 0 0.0   0
: 2     Inf 0.5 1 0.5 Inf
: 3     Inf 1.0 1 1.0 Inf
: 4     Inf 0.5 1 0.5 Inf
: bar     0 0.0 0 0.0 Inf

Is the matrix regular? And ergodic? Give me its canonic form:

#+attr_latex: :options style=vs, bgcolor=light-gray
#+begin_src R :session examples :results output :exports both
is.regular(mc)
is.irreducible(mc)
canonic <- canonicForm(mc)
canonic
#+end_src

#+RESULTS:
#+begin_example
[1] FALSE
[1] FALSE
Drunkard's walk
 A  5 - dimensional discrete Markov Chain defined by the following states:
 home, bar, 2, 3, 4
 The transition matrix  (by rows)  is defined as follows:
     home bar   2   3   4
home  1.0 0.0 0.0 0.0 0.0
bar   0.0 1.0 0.0 0.0 0.0
2     0.5 0.0 0.0 0.5 0.0
3     0.0 0.0 0.5 0.0 0.5
4     0.0 0.5 0.0 0.5 0.0
#+end_example

** Further work on the package
*** Mathematical tests for the package
Old way of testing algorithms in the package before this project took off was to take an example (or a
couple of examples) of reduced dimension from a book, find the analytical solutions, store them in a
variable, and check that the result of the methods coded in the package did not change against this
solution across different revisions of the software.

This has the problem that it lacks edge cases, and it is misses the point of testing a mathematical
property $\forall x P(x)$, because the $\forall$ becomes just a couple of examples.

We have designed a better approach for testing the package:

1. Still test against known examples taken from books (giving them credit for that). More examples in
this regard have been added.
1. Generate random Markov chains of random dimnension, given both by by-rows stochastic matrices and
by-column ones. We cannot generate non numerable infinite instances of course, to cover the $\forall$
that quantifies the property, but at least gets a little more closer than just a couple of examples.
1. Of course we cannot compute the solution for random instances by hand, so we have decided to check
against different implications, equivalences and characteristic equation systems inspired from the
chapter [[Markov chains]].
1. All algorithms coded in chapter [[=markovchain= package]] have been tested using this approach, and work
done in this sense can be consulted in the [[https://github.com/spedygiorgio/markovchain/tree/master/tests/testthat][testhat]] folder from the project.

Below we describe the structure of that folder and some of the most important tests carried out (those
tests have been made for a dataset of randomly generated Markov chains):

**** =setupData.R=
Generates random instances for general Markov chains, Markov chains with identity transition matrix
or Markov chains with strictly positive matrices.

**** =testHittingProbabilities.R=
+ All hitting probabilities are non negative
+ Hitting probabilities hold their characteristic system \eqref{eq:fij-recurrence}.
+ Hitting probabilities of an identity Markov chain is the identity of same dimension.

**** =testStatesClassification.R=
+ Test a state is absorbing iff it forms its own recurrent class.
+ Recurrent and transient states form a partition of states.
+ $f_{i,i} < 1$ for a transient state (by defintion this must hold).
+ All states are recurrent in a identity Markov chain.
+ If the Markov chain is ergodic $\implies$ all states are recurrent.
+ If there are transient states $\implies$ Markov chain is not be irreducible.
+ $f_{i,j} = 1$ for $i,j$ in same recurrent class.
+ $f_{i,k} = 0$ if $i$ is a recurrent state but $k$ does not belong to the same class.
+ Union of recurrent classes gives the recurrent states.
+ Union of transient classes gives the transient states.
+ Recurrent classes are disjoint.
+ Transient classes are disjoint.

**** =testSteadyStates.R=
+ Number of steady states should coincide with the number of recurrent classes.
+ A steady state is a probability vector.
+ Steady states for a matrix are linearly independent (since they should be a base of the convex hull).
+ Steady states are eigen vectors for the transition matrix.

**** =testCommClasses.R=
+ Communicating classes matrix is symmetric.
+ Rows of the same class can be permuted in a communicating classes matrix and the matrix does not change (i.e. if $i,j$ are in the same class, they should communicate with same states).
+ All classes are closed for identity matrices.
+ Communicating class matrix from \texttt{commClassesKernel} should coincide with the matrix arising from $(sign(P) + I)^{m - 1}$ where $sign(P)_{ij} = \twopartdefow{1}{p_{ij} > 0}{0}$ where $m$ is the number of states.
+ Communicating classes are a partition of the states of the Markov chain.

**** =testMarkovChainsClassification.R=
+ A transition matrix which is strictly positive is regular.
+ If a Markov chain is regular then it should be ergodic.
+ Check Perron-Frobenius theorem: a transition matrix is regular iff $1$ is the unique maximal eigen value
in modulo.
+ Markov chain is irreducible iff there is a single communicating class.
+ If the matrix is irreducible, its canonic form equals the Markov chain.
**** =testMeanMeasures.R=
+ Mean first passage times and mean recurrence times should hold the characteristic system \eqref{prop:mfpt-rec-mat}.
+ If we invert the mean recurrence times $(e_{11}^{-1}, \ldots, e_{mm}^{-1})$ is an eigen vector.
+ Mean number of visits should hold the characteristic system \eqref{eq:char-mean-number-visits}.
+ All mean number of visits are $\infty$ if the Markov chain is irreducible.
+ Absorption times hold the characteristic system \eqref{eq:char-absorption-probs}

*** Documentation
Further work on the package has been port its [[https://github.com/spedygiorgio/markovchain/tree/master/vignettes][vignettes]] (main form of documentation of an R package)
from =Sweave= to =knitr=, update and curate the documentation for the package or design a
web page [fn::[[https://spedygiorgio.github.io/markovchain/][https://spedygiorgio.github.io/markovchain/]]].

* Bibliography :ignore:
\bibliography{references}
