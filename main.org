#+TITLE: Cadenas de Markov
#+AUTHOR: Ignacio CordÃ³n Castillo
#+OPTIONS: toc:t num:3
#+STARTUP: indent
#+STARTUP: latexpreview

#+latex_class: scrreprt
#+latex_class_options: [oneside,english,openright,titlepage,numbers=noenddot,openany,headinclude,footinclude=true,cleardoublepage=empty,abstractoff,BCOR=5mm,paper=a4,fontsize=12pt,ngerman,american]
#+latex_header_extra: \definecolor{ugrColor}{HTML}{c6474b} 
#+latex_header_extra: \input{frontmatter/titlepage}\usepackage{wallpaper}\ThisULCornerWallPaper{1}{imgs/ugrA4.pdf}
#+latex_header_extra: \usepackage[T1]{fontenc}
#+latex_header_extra: \usepackage{babel}
#+latex_header_extra: \usepackage[beramono,eulerchapternumbers,linedheaders,parts,a5paper,dottedtoc,manychapters]{classicthesis}
#+latex_header_extra: \input{config/setup}
#+latex_header_extra: \input{config/classicthesis}
#+latex_header: \input{config/macros}

* Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

Markov chains are a mathematical area which encompasses probability, inference, algebra (more accurately graph theory), algorithms theory (since discrete markov chains can be simulated in a computer, and therefore, we intend to simulate them as efficiently as we can or use as big input sizes as we can, for example in markov chains with more states). All of the above conditions make markov chains an interesting study from both the mathematical and the computer science viewpoint.

* Introduction to Markov chains
** Definition
For convenience, unless otherwise indicated, we will consider $0\in \mathbb{N}$.

#+begin_definition
A stochastic random process $\{X_n\}_{n\in I}$, where $I\subseteq \mathbb{N}$, is a Markov chain iff the future process $\{X_k\}_{k > n, k \in I}$ is independent of the past process $\{X_k\}_{k < n, k \in I}$ conditionally on $X_n$.
#+end_definition

#+begin_definition
Given $\{X_n\}_{n\in I}$ a Markov chain, let us call the *state space* the set $S = \bigcup_{n \in I} Img \left(X_n\right)$, i.e. the set where the Markov chain takes its values.
#+end_definition

We will omit the subscript $n\in I$ and assume $I = \mathbb{N}$ henceforth. We will also assume $S$ is finite.

#+begin_lemma
$\{X_n\}_n$ is a Markov chain iff for all $n\in\mathbb{N}$ and given arbitrary $s_0, \ldots, s_{n + 1} \in S$, then it holds:

\[
P(X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0) = P(X_{n + 1} = s_{n + 1} | X_n = s_n)
\]
#+end_lemma

#+begin_proof

From left to right. By Markov chain definition, if $\{X_k\}_{k \in I}$ is independent of the past process $\{X_k\}_{m \in I}$ conditionally on $X_n$, then:

\begin{align*}
&P(X_{n + 1} = s_{n + 1}, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n) = \\
&P(X_{n + 1} = s_{n + 1} | X_n = s_n) \cdot P(X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)
\end{align*}

If we prove that 

\[
\frac{P(X_{n + 1} = s_{n + 1}, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)}{P(X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)} = P(X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0)
\]

then we would have arrived at our proof. That derives from:

\begin{align*}
\frac{P(X_{n + 1} = s_{n + 1}, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)}{P(X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)} &=
\frac{P(X_{n + 1} = s_{n + 1}, X_n = s_n, \ldots, X_0 = s_0)}{P(X_n = s_n, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0)} = \\
&= P(X_{n + 1}=s_{n + 1} | X_{n} = s_{n}, \ldots, X_0 = s_0)
\end{align*}

From right to left. We want to prove that:

\begin{align*}
&P(X_{n + m} = s_{n + m}, \ldots X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0) =\\
&P(X_{n + m} = s_{n + m}, \ldots, X_{n + 1} = s_{n + 1} | X_n = s_n)
\end{align*}

On the one hand, if the right part holds, it easy to prove that:

\begin{align*}
& P(X_0 = s_0, \ldots X_n = s_n ) =\\
& P(X_0 = s_0) \cdot P(X_1 = s_1 | X_0 = s_0) \cdot P(X_2 = s_2 | X_1 = s_1) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\end{align*}

substituting each:

\[
P(X_i = s_i | X_{i - 1} = s_{i - 1}) = P(X_i = s_i | X_{i - 1} = s_{i - 1}, \ldots, X_0 = s_0) = \frac{P(X_i = s_i, \ldots, X_0 = s_0)}{P(X_{i - 1} = s_{i - 1}, \ldots, X_0 = s_0)}
\]


Summing over all $s_0, \ldots s_{k - 1}$, we get:

\[
P(X_{k} = s_{k}, \ldots, X_n = s_n) = P(X_k = s_k) \cdot P(X_{k + 1} = s_{k + 1} | X_k = s_k) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\]

Therefore,

\begin{align*}
& P(X_{n + m} = s_{n + m}, \ldots, X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0) =\\
&= \frac{P(X_{n + m}, \ldots, X_0)}{P(X_{n + m - 1}, \ldots, X_0)} \cdot \frac{P(X_{n + m - 1}, \ldots, X_0)}{P(X_{n + m - 2}, \ldots, X_0)} \cdots \frac{P(X_{n + 1}, \ldots, X_0)}{P(X_{n}, \ldots, X_0)} = \\
&= P(X_{n + m} | X_{n + m - 1}) \cdots P(X_{n + 1} | X_{n}) = \frac{P(X_{n + m} = s_{n + m}, \ldots X_{n + 1} = s_{n + 1})}{P(X_n = s_n)} =\\
&= P(X_{n + m} = s_{n + m}, \ldots, X_{n + 1} = s_{n + 1} | X_n = s_n) 
\end{align*}
#+end_proof

** Chapman-Kolmogorov equation
As stated before, in the right to left implication (that now we can assume as characterization of Markov chains), we have shown that transition probabilities can be expressed as:

\begin{align}
& P(X_0 = s_0, \ldots X_n = s_n) = \nonumber \\
& P(X_0 = s_0) \cdot P(X_1 = s_1 | X_0 = s_0) \cdot P(X_2 = s_2 | X_1 = s_1) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\label{eq:trans-probs}
\end{align}

and also:

\begin{align}
& P(X_{m} = s_{m}, \ldots, X_n = s_n | X_{m - 1} = s_{m - 1})) = \nonumber \\
& = P(X_{m} = s_{m} | X_m = s_m) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\label{eq:trans-cond-probs}
\end{align}

#+begin_definition
Given $\{X_n\}_{n\in I}$ a Markov chain with finite state space $S$, let us define the *initial distribution* as $\mu(s) = P(X_0 = s), s \in S$.
#+end_definition

#+begin_definition
Given $\{X_n\}_{n\in I}$ a Markov chain with finite state space $S$, let us define the *transition probabilities* as 

\begin{align*}
& p_{s, t}(m, n) = P(X_n = t | X_m = s),\quad s,t \in S, \quad n,m \in \mathbb{N}, n\neq m \\ 
& p_{s, t}(n, n) = \mathbb{1}_{s = t}
\end{align*}
#+end_definition

Therefore, we can rewrite Equation \eqref{eq:trans-probs} as:

\[
P(X_k = s_k, k \le n) = \mu(s_0) \cdot p_{s_0, s_1}(0, 1) \cdot p_{s_1, s_2}(1, 2) \cdots p_{s_{n - 1}, s_n}(n - 1, n)
\]

Summing over $m - 1, \ldots, n -1$ in Equation \eqref{eq:trans-cond-probs}, we deduce:

\begin{equation}
p_{s, t}(m, n) = \sum_{s^{(1)} \in S} \ldots \sum_{s^{(n - m - 1)} \in S} p_{s, s^{(1)}}(m, m + 1) \cdots p_{s^{(n - m - 1)}, t}(n - 1, n)
\label{eq:sum-probs}
\end{equation}

#+begin_definition 
We will define the matrix $P(m, n) = (p_{s, t}(m, n))_{s, t \in S}$ henceforth.
#+end_definition

#+begin_proposition
$P$ verifies the semigroup property, best known as *Chapman-Kolmogorov equation*:

\[
P(m, n) = P(m, l) \cdot P(l, n), \quad \forall m \le l \le n 
\]
#+end_proposition
#+begin_proof
Straightforward from Equation \eqref{eq:sum-probs} 
#+end_proof

** Time-homogeneous Markov chains
#+begin_definition
Given a Markov chain $\{X_n\}$ with finite space state $S=\{0, \ldots, k\}$, it is said to be a *time-homogeneous Markov chain* iff it holds:

\[
\forall s, t\in S, \forall n \in \mathbb{N} \qquad p_{s, t} (n, n + 1) = p_{s, t} \in [0, 1]
\]

(i.e. the transition probabilities do not depend on the time).

$p_{s, t}$ is called *one-step transition probability* and $P = (p_{s, t})_{s, t \in S}$ the transition matrix.
#+end_definition

#+begin_proposition
It holds $P(m, n) = P^{n - m}$
#+end_proposition

#+begin_proof
Trivial
#+end_proof


#+begin_proposition
Naming $\mu_n = \left(\mu_n(0), \ldots, \mu_n(k)\right)$, it holds:

\[
\mu_n = \mu_0 \cdot P^n
\]
#+end_proposition

#+begin_proof
Trivial from Equation \eqref{eq:trans-probs}
#+end_proof

#+begin_definition
We define the law of a Markov chain $\{X_n\}$, where $X_n: \Omega \longrightarrow \mathbb{S}$, as the probability distribution of $X: \Omega \longrightarrow S^{\mathbb{N}}$, where $X(\omega) = \{X_n(w)\}_{n \in \mathbb{N}}$.
#+end_definition

#+begin_definition
We will denote:

\[
P_s(A) = P(A | X_0 = s)
\]

for any given state $s\in S$. Let's note that $P_s$ is univocally defined by $P_s(X_k = t) \quad \forall k \ge 0, t \in S$.

We will also denote

\[
\mathbb{E}_s Y = \mathbb{E}(Y | X_0 = s) = \sum_y y P(Y = y | X_0 = s) = \sum_y y P_i(Y = y)
\]
#+end_definition

** Stationarity
#+begin_definition
We say that $\{X_n\}$ is stationary iff $\{X_n\}_{n \ge 0}$ has the same law as $\{X_n\}_{n \ge m}$.
#+end_definition

#+begin_proposition
A Markov chain $\{X_n\}$ is stationary iff it is time homogeneous and all the $X_k$ have the same 
distribution.
#+end_proposition

#+begin_proof
From right to left, if $\{X_n\}$ is stationary, $P(X_0 = s) = P(X_m = s)$ for all $s\in S, m\in \mathbb{N}$
#+end_proof

We will assume all the Markov chains as time-homogeneous ones henceforth, unless stated 
otherwise.
** Stopping times and strong Markov property
#+begin_definition
Given $\{X_n\}$ a Markov chain, defined over $(\Omega, \mathcal{F}, P)$, then a *stopping time* is a random variable 
$\tau : \Omega \mapsto \mathbb{N} \bigcap \{\infty\}$ such that:

\[
\{\tau = n \} \in \mathcal{F}_n = \sigma(X_0, X_1, \ldots, X_n), \quad \forall n \in \mathbb{N}
\]

Moreover, we call the $\sigma$ -algebra of events previous to $\tau$, namely $\mathcal{F}_{\tau}$, the one which contains all
the events $A \in \mathcal{F}$ following:

\[
A\bigcap \{\tau = n \} \in \mathcal{F}_n
\]
#+end_definition

For example, in the case of a coin, the first instant (or coin toss) $\tau$ where we get 4 heads 
in a row, is a valid stopping time, because we only have to account for the previous random 
variables; whereas the first toss where we are just one toss away from having obtained 4 heads 
is not a valid stopping time, since it involves future events.

Given $S = \{i_1, i_2, \ldots\}$ we would denote $S^m = \{(i^{(1)}, \ldots, i^{(m)}): i^{(j)} \in S\}$.

#+begin_proposition
Let $\{X_n\}$ be a Markov chain with $S$ as state space and stationary transition probabilities.
Let $\tau$ be a stopping time. Then for all $m\in \mathbb{N}, C \subseteq S^m, A \in \mathcal{F}_\tau,
i \in S$, if $P(A \bigcap \{X_\tau = i\}) > 0$, then it holds:

\begin{equation}
P\left((X_{\tau + 1}, \ldots, X_{\tau + m}) \in C | A, X_{\tau} = i\right) = 
P\left((X_{\tau + 1}, \ldots, X_{\tau + m}) \in C | X_{\tau} = i\right)
\label{ec:strong-mc}
\end{equation}
#+end_proposition

#+begin_proof
Let us start taking $m = 1, C = \{j\}$. Then, intersecting over $\{\tau = n\}$, we have:

\begin{align*}
P\left(X_{\tau + 1} = j, A, X_{\tau} = i\right) &=
\sum_{n = 0}^{\infty} P\left(\tau = n, X_{n + 1}, A, X_n = i\right) = \\
&= \sum_{n = 0}^{\infty} P\left(\tau = n, A, X_n = i\right) \cdot P\left(X_{n + 1} = j | \tau = n, A, X_n = i\right) = \\
&= \sum_{n = 0}^{\infty} P\left(\tau = n, A, X_n = i\right) \cdot P\left(X_{n + 1} = j | X_n = i\right) = \\
&= p_{i,j} \sum_{n = 0}^{\infty} P\left(\tau = n, A, X_n = i\right) = p_{i, j} P\left(A, X_{\tau} = i\right)
\end{align*}

where we have used $A\bigcap \{\tau = n\} \in \mathcal{F}_n$ to simplify 
$P\left(X_{n + 1} = j | \tau = n, A, X_n = i\right)$
as $P\left(X_{n + 1} = j | X_n = i\right)$. Therefore, using the proven equality we can show:

\[
P\left(X_{\tau + 1} = j | A, X_\tau = i\right) = \frac{P\left(X_{\tau + 1} = j, A, X_{\tau} = i\right)}{P(A, X_{\tau} = i)} = p_{i, j}
\]

Applying several times this argument, we have:

\begin{align*}
& P\left(X_{\tau + 1} = i_1, X_{\tau + 2} = i_2, \ldots, X_{\tau + m} = i_m | A, X_{\tau} = i\right) =\\
&= P\left(X_{\tau + 1} = i_1 | A, X_{\tau} = i\right) \cdot P\left(X_{\tau + 2} = i_2 | A, X_{\tau} = i, X_{\tau + 1} = i_1\right) \cdots \\
&\cdots P\left(X_{\tau + m} = i_m | A, X_{\tau} = i, X_{\tau + 1} = i_1, \ldots X_{\tau + m - 1} = i_{m - 1}\right) = p_{i, i_1} p_{i_1, i_2} \cdots p_{i_{m - 1}, i_m}
\end{align*}

In particular, when $A$ is the whole space $\Omega$, we arrive at:

\[
P\left(X_{\tau + 1} = i_1, X_{\tau + 2} = i_2, \ldots, X_{\tau + m} = i_m | X_{\tau} = i\right) = p_{i, i_1} p_{i_1, i_2} \cdots p_{i_{m - 1}, i_m}
\]

which shows the equality:

\[
P\left(X_{\tau + 1} = i_1, \ldots, X_{\tau + m} = i_m | A, X_{\tau} = i\right) =
P\left(X_{\tau + 1} = i_1, \ldots, X_{\tau + m} = i_m | X_{\tau} = i\right) 
\]

We have shown that Equation \eqref{ec:strong-mc} holds for $C = \{(i_1, \ldots, i_m)\}$. For
$C \subseteq S^m$ it would suffice summing over $(i_1, \ldots, i_m) \in C$.
#+end_proof

The previous result says us that all the Markov chains (in discrete time) have the strong Markov 
property, and it can be interpreted as the Markov chain restarting each time it hits the state 
$i \in S$.

** Hitting times
The probabilities $p_{i, j}^{(n)} = P_i\left(X_n = j\right)$ gives us of the probability of the
chain reaching state $j$ in the $n$ -th period, if it starts from $i$. But the state $j$ could have
been visited before. We are interested now in finding the first time that a given state is reached.

#+begin_definition
We define the *hitting time* for the state $j \in S$ as:

\[
\tau_j = min \{n > 0 : X_n = j \}
\] 
#+end_definition

#+begin_definition

We call *first hitting time probability of the state $j$*, provided that the chain starts in the 
state $i$, to:

\[
f_{i,j}^{(n)} = P_i\left(\tau_j = n\right) = P_i\left(X_n = j, X_m \neq j \quad \forall m = 1, \ldots, n - 1 \right)
\]

and we will denote $f_{i, j} := \sum_{n = 0}^{\infty} f_{i,j}^{(n)} = P_i\left(\tau_j < \infty\right)$
#+end_definition

Note that we coul have $f_{i, j} < 1$ iff $P_i\left(X_n \neq j \forall n \ge 1\right) > 0$.

#+begin_definition
We will call *return time distribution* for the state $i$ to $f_{i,i}^{(n)} = P_i \left(\tau_i = n\right)$, 
and we will denote $f_{i,i} = P_i\left(\tau_i < \infty\right)$ (i.e. the probability of ever returning 
to the start state $i$.
#+end_definition

*** Computation through absorption
Let us consider a fixed state $j$. The aim is to compute $f_{i,j}^{(n)}$ with $i\neq j$. 
We could freeze the chain $\{X_n\}$ once it hits $j$ in that state, that is, taking:

\[
\tildemc = \twopartdef 
             {X_n} { X_r \neq j \; \forall r < n } 
             {j} {X_r = j \textrm{ for some } r  < n}
\]

That is, $j$ has been transformed into an absorbing state, such that $\tildemc$ cannot leave.

The transition matrix for $\tildemc$ is given by:

\[
\tildeprob_{i, k} = \probi{\tildex_1 = k} = 
  \threepartdef 
    { p_{i,k} } { i\neq j }
    { 0 }       { i = j, k \neq j }
    { 1 }       { i = j, k = j }
\]

That is, the transition matrix $\tildeP$ has the same rows as $P$, except for the $j$ -th row, 
which has been substituted by $(0, \ldots, 0, \underset{j}{1}, 0, \ldots, 0)$. Therefore 
we have that the $i, j$ entry for the matrix $\tildeP^n$ is:

\[
\tildep_{i,j}^{(n)} = \probi{\tildemc = j} = \probi{\tau_j \le n}
\]

and hence:

\[
f_{i,j}^{(n)} = \probi{\tau_j \le n} - \probi{\tau_j \le n - 1} = \tildep_{i, j}^{(n)} - \tildep_{i, j}^{(n - 1)}
\]

So obtaining $f_{i,j}^{(n)}$ can be done by computing $\tildeP^{n - 1}$ and $\tildeP^n$, 
except for $j = i$. To compute that last probability, we could use:

\[
f_{jj}^{(n)} = \twopartdef
  { p_{jj} } { n = 1 }
  { \sum_{k \in S, k \neq j} p_{j,k} f_{k,j}^{(n - 1)} } { n \ge 2 }
\]

\begin{example}
We consider a Markov chain with states $S = \{1,2,3\}$ and the following transition matrix:

\[
  P = \left(
  \begin{array}{ccc}
  \frac{1}{3} & \frac{2}{3} &      0      \\
  \frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
      0       & \frac{3}{4} & \frac{1}{4}
  \end{array}
  \right)
\]

To compute the time it takes us to reach the state $3$, we should consider:

\[
\tildeP = \left(
  \begin{array}{ccc}
  \frac{1}{3} & \frac{2}{3} & 0 \\
  \frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
       0      &      0      &   1
  \end{array}
\right) = \frac{1}{10} 
\threemat {1 & 2 & 4} {1 & -1 & 3} {1 & 0 & 0} \cdot
\threemat {1 &   &  } {  &  0 &  } {  &   & \frac{5}{6}} \cdot
\threemat {0 & 0 & 10}{3 & -4 & 1} {1 & 2 &-3}
\]

It is easy to show, calling $\alpha = \frac{5}{6}$:

\[
\tilde{P}^n = \threemat
            { \frac{2}{5} \alpha^n & \frac{4}{5} \alpha^n & 1 - \alpha^{n - 1}}
            { \frac{3}{10}\alpha^n & \frac{3}{5} \alpha^n & 1 - \frac{3}{4}\alpha^{n - 1} }
            { 0 & 0 & 1 }
\]

Therefore:

\[
P_1 \left(\tau_3 = n\right) = \tildep_{1,3}^{(n)} - \tildep_{1,3}^{(n-1)} =
1 - \alpha^{n - 1} -  (1 - \alpha^{n - 2}) = (1 - \alpha) \alpha^{n - 2} = \frac{1}{6}\alpha^{n - 2} n \ge 2
\]

\[
P_2 \left( \tau_3 = n \right) = \twopartdef { \frac{1}{4} } { n = 1 }
                                            { \tildep_{2,3}^{(n)} - \tildep_{2,3}^{(n-1)} = \frac{1}{8} \alpha^{n - 2} } { n\ge 2 }
\]

To go from $3$ to itself, we could go directly or passing by $2$ (we could not go through
$1$ since $p_{3,1} = 0$:

\[
P_3\left(\tau_3 = n\right) = \threepartdef
  { \frac{1}{4} } { n = 1 } 
  { \frac{3}{16} } { n = 2 }
  { \frac{3}{32} \alpha^{n - 3} } { n \ge 3 }
\]
\end{example}

*** Computation through transition probabilities
We could ask ourselves whether it is possible to relate $f_{i,j}^{(n)}$ to $p_{i,j}^{(n)}$
directly. It holds:

\begin{align}
p_{i,j}^{(n)} &= \probi{X_n = j} = \sum_{m = 1}^{n} \probi{\tau_j = m} 
\probi{X_n = j | \tau_j = = m} = \nonumber \\
&= \sum_{m = 1}^{n} \probi{\tau_j = m} \probj{X_{n-m} = j} = 
\sum_{m = 1}^n f_{i, j}^{(m)} p_{j,j}^{(n - m)}
\label{eq:ps-fs-relation}
\end{align}   

It is not possible to express $f_{i,j}^{(n)}$ in terms of $p_{i,j}^{(n)}$, but taking the following generating functions, we can determine a theoretical relationship between both:

\[
P_{i,j}(s) = \sum_{n = 1}^{\infty} p_{i,j}^{(n)} s^n \qquad 
F_{i,j}(s) = \sum_{n = 1}^{\infty} f_{i,j}^{(n)} s^n
\]

According to $\eqref{eq:ps-fs-relation}$:

\[
P_{i,j}(s) = \sum_{n = 1}^{\infty} \sum_{m = 1}^n f_{i, j}^{(m)} p_{j,j}^{(n - m)} s^n = 
\sum_{n = 1}^{\infty} \sum_{m = 1}^{n - 1} f_{i, j}^{(m)} p_{j,j}^{(n - m)} s^n + 
\sum_{n = 1}^{\infty} f_{i, j}^{(n)} \underbrace{p_{j,j}^{(0)}}_{1} s^n \underset{(\ast)} = F_{i,j}(s) P_{j,j}(s) + F_{i,j}(s)
\]

where in $(\ast)$ it has been used that $\sum_{n = 1}^{\infty} \sum_{m = 1}^{\infty} p_{i, j}^{(n)} f_{i,j}^{(m)} s^{n + m}$ can be rewritten as (fixes $k$, there we can take finite $m$ positive values and $k - m$ positive):

\[
\sum_{n = 1}^{\infty} \sum_{m = 1}^{n - 1} f_{i, j}^{(m)} p_{j,j}^{(n - m)} s^n
\]

We deduce:

\[
F_{i,j}(s) = \frac{P_{i,j}(s)}{1 + P_{j,j}(s)}
\]

Although this is just a theoretical relationship for the moment, we can observe, on the one hand:

\[
F_{i,j}(1) = f_{i,j} = \probi{\tau_j < \infty}
\]
and if the condition $f_{i,j} = 1$ is met:

\[
F_{i,j}'(j) = \sum_{n = 1}^{\infty} n f_{i,j}^{(n)} = \expecti{\tau_j}
\]

#+begin_definition
Taking a fixed state $j$, we can define:

1. The *expected number of visited to $j$ * as $P_{i,j}(1) = \sum_{n = 1}^{\infty} p_{i,j}^{(n)}$
2. The *total number of visits to $j$ * as $V_j = \sum_{n = 1}^{\infty} \mathbb{1}_{X_n = j}$
#+end_definition

#+begin_proposition
It holds that $\expecti{V_j} = P_{i,j}(1)$
#+end_proposition

#+begin_proof
Straightforward from $\expecti{V_j} = \sum_{n = 1}^{\infty} \expecti{\mathbb{1}_{X_n = j}} = \sum_{n = 1}^{\infty} p_{i,j}^{(n)}$
#+end_proof

* Bibliography
:PROPERTIES:
:UNNUMBERED: t
:END:
