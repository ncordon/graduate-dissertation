#+TITLE: Cadenas de Markov
#+AUTHOR: Ignacio CordÃ³n Castillo
#+OPTIONS: toc:t num:3
#+STARTUP: indent
#+STARTUP: latexpreview

#+latex_class: scrreprt
#+latex_class_options: [oneside,english,openright,titlepage,numbers=noenddot,openany,headinclude,footinclude=true,cleardoublepage=empty,abstractoff,BCOR=5mm,paper=a4,fontsize=12pt,ngerman,american]
#+latex_header_extra: \definecolor{ugrColor}{HTML}{c6474b} 
#+latex_header_extra: \input{titlepage}\usepackage{wallpaper}\ThisULCornerWallPaper{1}{ugrA4.pdf}
#+latex_header_extra: \usepackage[T1]{fontenc}
#+latex_header_extra: \usepackage{babel}
#+latex_header_extra: \usepackage[beramono,eulerchapternumbers,linedheaders,parts,a5paper,dottedtoc,manychapters]{classicthesis}
#+latex_header_extra: \input{setup}
#+latex_header_extra: \input{classicthesis-config}
#+latex_header: \input{macros}

* Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

Markov chains are a mathematical area which encompasses probability, inference, algebra (more accurately graph theory), algorithms theory (since discrete markov chains can be simulated in a computer, and therefore, we intend to simulate them as efficiently as we can or use as big input sizes as we can, for example in markov chains with more states). All of the above conditions make markov chains an interesting study from both the mathematical and the computer science viewpoint.

* Introduction to Markov chains
** Definition
For convenience, unless otherwise indicated, we will consider $0\in \mathbb{N}$.

#+begin_definition
A stochastic random process $\{X_n\}_{n\in I}$, where $I\subseteq \mathbb{N}$, is a Markov chain iff the future process $\{X_k\}_{k > n, k \in I}$ is independent of the past process $\{X_k\}_{k < n, k \in I}$ conditionally on $X_n$.
#+end_definition

#+begin_definition
Given $\{X_n\}_{n\in I}$ a Markov chain, let us call the *state space* the set $S = \bigcup_{n \in I} Img \left(X_n\right)$, i.e. the set where the Markov chain takes its values.
#+end_definition

We will omit the subscript $n\in I$ and assume $I = \mathbb{N}$ henceforth. We will also assume $S$ is finite.

#+begin_lemma
$\{X_n\}_n$ is a Markov chain iff for all $n\in\mathbb{N}$ and given arbitrary $s_0, \ldots, s_{n + 1} \in S$, then it holds:

\[
P(X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0) = P(X_{n + 1} = s_{n + 1} | X_n = s_n)
\]
#+end_lemma

#+begin_proof

From left to right. By Markov chain definition, if $\{X_k\}_{k \in I}$ is independent of the past process $\{X_k\}_{m \in I}$ conditionally on $X_n$, then:

\begin{align*}
&P(X_{n + 1} = s_{n + 1}, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n) = \\
&P(X_{n + 1} = s_{n + 1} | X_n = s_n) \cdot P(X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)
\end{align*}

If we prove that 

\[
\frac{P(X_{n + 1} = s_{n + 1}, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)}{P(X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)} = P(X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0)
\]

then we would have arrived at our proof. That derives from:

\begin{align*}
\frac{P(X_{n + 1} = s_{n + 1}, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)}{P(X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0 | X_n = s_n)} &=
\frac{P(X_{n + 1} = s_{n + 1}, X_n = s_n, \ldots, X_0 = s_0)}{P(X_n = s_n, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0)} = \\
&= P(X_{n + 1}=s_{n + 1} | X_{n} = s_{n}, \ldots, X_0 = s_0)
\end{align*}

From right to left. We want to prove that:

\begin{align*}
&P(X_{n + m} = s_{n + m}, \ldots X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0) =\\
&P(X_{n + m} = s_{n + m}, \ldots, X_{n + 1} = s_{n + 1} | X_n = s_n)
\end{align*}

On the one hand, if the right part holds, it easy to prove that:

\begin{align*}
& P(X_0 = s_0, \ldots X_n = s_n ) =\\
& P(X_0 = s_0) \cdot P(X_1 = s_1 | X_0 = s_0) \cdot P(X_2 = s_2 | X_1 = s_1) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\end{align*}

substituting each:

\[
P(X_i = s_i | X_{i - 1} = s_{i - 1}) = P(X_i = s_i | X_{i - 1} = s_{i - 1}, \ldots, X_0 = s_0) = \frac{P(X_i = s_i, \ldots, X_0 = s_0)}{P(X_{i - 1} = s_{i - 1}, \ldots, X_0 = s_0)}
\]


Summing over all $s_0, \ldots s_{k - 1}$, we get:

\[
P(X_{k} = s_{k}, \ldots, X_n = s_n) = P(X_k = s_k) \cdot P(X_{k + 1} = s_{k + 1} | X_k = s_k) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\]

Therefore,

\begin{align*}
& P(X_{n + m} = s_{n + m}, \ldots, X_{n + 1} = s_{n + 1} | X_n = s_n, \ldots, X_0 = s_0) =\\
&= \frac{P(X_{n + m}, \ldots, X_0)}{P(X_{n + m - 1}, \ldots, X_0)} \cdot \frac{P(X_{n + m - 1}, \ldots, X_0)}{P(X_{n + m - 2}, \ldots, X_0)} \cdots \frac{P(X_{n + 1}, \ldots, X_0)}{P(X_{n}, \ldots, X_0)} = \\
&= P(X_{n + m} | X_{n + m - 1}) \cdots P(X_{n + 1} | X_{n}) = \frac{P(X_{n + m} = s_{n + m}, \ldots X_{n + 1} = s_{n + 1})}{P(X_n = s_n)} =\\
&= P(X_{n + m} = s_{n + m}, \ldots, X_{n + 1} = s_{n + 1} | X_n = s_n) 
\end{align*}
#+end_proof

** Chapman-Kolmogorov equation
As stated before, in the right to left implication (that now we can assume as characterization of Markov chains), we have shown that transition probabilities can be expressed as:

\begin{align}
& P(X_0 = s_0, \ldots X_n = s_n) = \nonumber \\
& P(X_0 = s_0) \cdot P(X_1 = s_1 | X_0 = s_0) \cdot P(X_2 = s_2 | X_1 = s_1) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\label{eq:trans-probs}
\end{align}

and also:

\begin{align}
& P(X_{m} = s_{m}, \ldots, X_n = s_n | X_{m - 1} = s_{m - 1})) = \nonumber \\
& = P(X_{m} = s_{m} | X_m = s_m) \cdots P(X_n = s_n | X_{n - 1} = s_{n - 1})
\label{eq:trans-cond-probs}
\end{align}

#+begin_definition
Given $\{X_n\}_{n\in I}$ a Markov chain with finite state space $S$, let us define the *initial distribution* as $\mu(s) = P(X_0 = s), s \in S$.
#+end_definition

#+begin_definition
Given $\{X_n\}_{n\in I}$ a Markov chain with finite state space $S$, let us define the *transition probabilities* as 

\begin{align*}
& p_{s, t}(m, n) = P(X_n = t | X_m = s),\quad s,t \in S, \quad n,m \in \mathbb{N}, n\neq m \\ 
& p_{s, t}(n, n) = \mathbb{1}_{s = t}
\end{align*}
#+end_definition

Therefore, we can rewrite Equation \eqref{eq:trans-probs} as:

\[
P(X_k = s_k, k \le n) = \mu(s_0) \cdot p_{s_0, s_1}(0, 1) \cdot p_{s_1, s_2}(1, 2) \cdots p_{s_{n - 1}, s_n}(n - 1, n)
\]

Summing over $m - 1, \ldots, n -1$ in Equation \eqref{eq:trans-cond-probs}, we deduce:

\begin{equation}
p_{s, t}(m, n) = \sum_{s^{(1)} \in S} \ldots \sum_{s^{(n - m - 1)} \in S} p_{s, s^{(1)}}(m, m + 1) \cdots p_{s^{(n - m - 1)}, t}(n - 1, n)
\label{eq:sum-probs}
\end{equation}

#+begin_definition 
We will define the matrix $P(m, n) = (p_{s, t}(m, n))_{s, t \in S}$ henceforth.
#+end_definition

#+begin_proposition
$P$ verifies the semigroup property, best known as *Chapman-Kolmogorov equation*:

\[
P(m, n) = P(m, l) \cdot P(l, n), \quad \forall m \le l \le n 
\]
#+end_proposition
#+begin_proof
Straightforward from Equation \eqref{eq:sum-probs} 
#+end_proof

** Time-homogeneous Markov chains
#+begin_definition
Given a Markov chain $\{X_n\}$ with finite space state $S=\{0, \ldots, k\}$, it is said to be a *time-homogeneous Markov chain* iff it holds:

\[
\forall s, t\in S, \forall n \in \mathbb{N} \qquad p_{s, t} (n, n + 1) = p_{s, t} \in [0, 1]
\]

(i.e. the transition probabilities do not depend on the time).

$p_{s, t}$ is called *one-step transition probability* and $P = (p_{s, t})_{s, t \in S}$ the transition matrix.
#+end_definition

#+begin_proposition
It holds $P(m, n) = P^{n - m}$
#+end_proposition

#+begin_proof
Trivial
#+end_proof


#+begin_proposition
Naming $\mu_n = \left(\mu_n(0), \ldots, \mu_n(k)\right)$, it holds:

\[
\mu_n = \mu_0 \cdot P^n
\]
#+end_proposition

#+begin_proof
Trivial from Equation \eqref{eq:trans-probs}
#+end_proof

#+begin_definition
We define the law of a Markov chain $\{X_n\}$ as its successive joint distributions, i.e.

\[
l(n, s_n, s_{n - 1}, \ldots, s_0) = P(X_n = s_n, X_{n - 1} = s_{n - 1}, \ldots, X_0 = s_0) \quad \forall n \in \mathbb{N}, s_k \in S
\]
#+end_definition

#+begin_definition
We will denote:

\[
P_s(A) = P(A | X_0 = s)
\]

for any given state $s\in S$. Let's note that $P_s$ is univocally defined by $P_s(X_k = t) \quad \forall k \ge 0, t \in S$.

We will also denote

\[
\mathbb{E}_s Y = \mathbb{E}(Y | X_0 = s) = \sum_y y P(Y = y | X_0 = s) = \sum_y y P_i(Y = y)
\]
#+end_definition

** Stationarity
#+begin_definition
We say that $\{X_n\}$ is stationary iff $\{X_n\}_{n \ge 0}$ has the same law as $\{X_n\}_{n \ge m}$.
#+end_definition

We will assume all the Markov chains as time-homogeneous ones henceforth, unless stated otherwise.
* Second chapter
* Bibliography
